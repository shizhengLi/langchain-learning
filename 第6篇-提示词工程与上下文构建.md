# 第6篇：提示词工程与上下文构建

## 摘要

本文深入探讨了RAG系统中的提示词工程技术和上下文构建策略。通过分析提示词设计原则、上下文压缩技术、链式调用模式以及Agent应用，帮助读者掌握构建高效RAG系统的核心技术，提升大语言模型的回答质量和可控性。

## 1. 提示词工程基础

### 1.1 提示词在RAG中的作用

提示词工程是RAG系统中连接检索结果和大语言模型的桥梁，其质量直接影响最终回答的效果。一个好的提示词应该：

- **明确角色**：告诉AI它的身份和职责
- **提供上下文**：包含相关的检索结果
- **设定约束**：明确回答的格式和限制
- **引导输出**：指定期望的回答风格和内容

```
检索文档 → 上下文构建 → 提示词模板 → LLM生成 → 最终答案
```

### 1.2 提示词设计原则

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
import re
import json
from jinja2 import Template

class PromptType(Enum):
    """提示词类型"""
    QA = "qa"                    # 问答
    SUMMARIZATION = "sum"        # 摘要
    COMPARISON = "comp"          # 对比
    EXPLANATION = "exp"          # 解释
    GENERATION = "gen"           # 生成
    TRANSLATION = "trans"        # 翻译

@dataclass
class PromptTemplate:
    """提示词模板"""
    name: str
    template: str
    type: PromptType
    variables: List[str] = field(default_factory=list)
    description: str = ""
    examples: List[Dict[str, Any]] = field(default_factory=list)

class BasePromptBuilder(ABC):
    """提示词构建器基类"""

    @abstractmethod
    def build_prompt(self,
                    context: List[str],
                    question: str,
                    **kwargs) -> str:
        """构建提示词"""
        pass

    @abstractmethod
    def get_template_name(self) -> str:
        """获取模板名称"""
        pass

class QAPromptBuilder(BasePromptBuilder):
    """问答提示词构建器"""

    def __init__(self,
                 style: str = "formal",
                 language: str = "chinese",
                 include_sources: bool = True):
        self.style = style
        self.language = language
        self.include_sources = include_sources

        # 模板定义
        self.templates = self._load_templates()

    def _load_templates(self) -> Dict[str, str]:
        """加载提示词模板"""
        templates = {
            "formal_chinese": """
你是一个专业的AI助手，专门基于提供的文档内容回答用户问题。请遵循以下原则：

1. 仅基于提供的文档内容回答问题
2. 如果文档中没有相关信息，请明确说明
3. 回答要准确、简洁、结构清晰
4. 引用具体的文档内容支持你的回答

文档内容：
{context}

用户问题：
{question}

请基于以上文档内容回答用户问题：
""",

            "casual_chinese": """
我是一个AI助手，现在要根据下面的资料来回答你的问题。

参考资料：
{context}

你的问题是：
{question}

根据资料，我的回答是：
""",

            "technical_chinese": """
作为技术专家，请基于以下技术文档回答问题：

技术文档：
{context}

问题：{question}

要求：
- 提供准确的技术解释
- 如有步骤，请详细说明
- 代码示例请用代码块格式
- 如果文档不足，请说明需要哪些额外信息

回答：
""",

            "formal_english": """
You are a professional AI assistant that answers questions based on provided documents. Please follow these principles:

1. Answer only based on the provided document content
2. If information is not available in the documents, state clearly
3. Provide accurate, concise, and well-structured answers
4. Cite specific document content to support your answer

Document Content:
{context}

User Question:
{question}

Please answer based on the above document content:
"""
        }

        return templates

    def build_prompt(self,
                    context: List[str],
                    question: str,
                    **kwargs) -> str:
        """构建问答提示词"""
        # 合并上下文
        context_text = self._format_context(context, **kwargs)

        # 选择模板
        template_key = f"{self.style}_{self.language}"
        template = self.templates.get(template_key, self.templates["formal_chinese"])

        # 添加源信息
        if self.include_sources:
            template += "\n\n参考来源：请基于提供的文档内容回答，并在回答中引用相关信息。"

        # 渲染模板
        rendered_prompt = template.format(
            context=context_text,
            question=question
        )

        return rendered_prompt.strip()

    def _format_context(self, context: List[str], **kwargs) -> str:
        """格式化上下文"""
        max_length = kwargs.get('max_context_length', 4000)
        separator = kwargs.get('context_separator', '\n\n---\n\n')

        # 控制上下文长度
        context_text = separator.join(context)
        if len(context_text) > max_length:
            # 智能截断：尽量保持完整段落
            truncated_text = context_text[:max_length]
            last_separator = truncated_text.rfind(separator)
            if last_separator > max_length * 0.8:  # 如果截断点合理
                context_text = truncated_text[:last_separator]
            else:
                context_text = truncated_text + "..."

        return context_text

    def get_template_name(self) -> str:
        return f"QA_{self.style}_{self.language}"

class ChainOfThoughtPromptBuilder(BasePromptBuilder):
    """思维链提示词构建器"""

    def __init__(self, language: str = "chinese"):
        self.language = language
        self.templates = self._load_cot_templates()

    def _load_cot_templates(self) -> Dict[str, str]:
        """加载思维链模板"""
        return {
            "chinese": """
请按照以下步骤基于提供的文档内容回答问题：

步骤1：理解问题
- 明确用户想了解什么
- 识别关键概念和要求

步骤2：分析文档
- 找到相关的文档片段
- 理解文档中的关键信息

步骤3：推理分析
- 基于文档信息进行逻辑推理
- 考虑不同角度和可能性

步骤4：组织答案
- 提供清晰、结构化的回答
- 引用具体的文档内容

文档内容：
{context}

用户问题：{question}

请按照上述步骤逐步思考并回答：

步骤1：问题理解
"""

    def build_prompt(self,
                    context: List[str],
                    question: str,
                    **kwargs) -> str:
        """构建思维链提示词"""
        context_text = '\n\n---\n\n'.join(context)
        template = self.templates.get(self.language, self.templates["chinese"])

        return template.format(
            context=context_text,
            question=question
        ).strip()

    def get_template_name(self) -> str:
        return f"CoT_{self.language}"

class FewShotPromptBuilder(BasePromptBuilder):
    """少样本提示词构建器"""

    def __init__(self,
                 examples: List[Dict[str, str]] = None,
                 language: str = "chinese"):
        self.examples = examples or []
        self.language = language

    def add_example(self, context: str, question: str, answer: str):
        """添加示例"""
        self.examples.append({
            'context': context,
            'question': question,
            'answer': answer
        })

    def build_prompt(self,
                    context: List[str],
                    question: str,
                    **kwargs) -> str:
        """构建少样本提示词"""
        prompt_parts = []

        # 添加指令
        if self.language == "chinese":
            prompt_parts.append("请参考以下示例，基于提供的文档内容回答问题：\n")
        else:
            prompt_parts.append("Please refer to the following examples and answer based on the provided documents:\n")

        # 添加示例
        for i, example in enumerate(self.examples, 1):
            prompt_parts.append(f"示例 {i}:")
            prompt_parts.append(f"文档: {example['context']}")
            prompt_parts.append(f"问题: {example['question']}")
            prompt_parts.append(f"回答: {example['answer']}\n")

        # 添加实际问题和上下文
        prompt_parts.append("现在请回答以下问题:")
        prompt_parts.append(f"文档: {'\n\n---\n\n'.join(context)}")
        prompt_parts.append(f"问题: {question}")
        prompt_parts.append("回答:")

        return '\n'.join(prompt_parts)

    def get_template_name(self) -> str:
        return f"FewShot_{self.language}"
```

## 2. 上下文构建与优化

### 2.1 上下文压缩技术

```python
class ContextCompressor:
    """上下文压缩器"""

    def __init__(self, max_tokens: int = 3000, compression_ratio: float = 0.7):
        self.max_tokens = max_tokens
        self.compression_ratio = compression_ratio
        self.llm_client = None

    def compress_context(self,
                        context_documents: List[str],
                        query: str,
                        method: str = "relevance") -> List[str]:
        """压缩上下文"""
        if method == "relevance":
            return self._relevance_based_compression(context_documents, query)
        elif method == "llm":
            return self._llm_based_compression(context_documents, query)
        elif method == "hybrid":
            return self._hybrid_compression(context_documents, query)
        else:
            return context_documents[:int(len(context_documents) * self.compression_ratio)]

    def _relevance_based_compression(self,
                                    context_documents: List[str],
                                    query: str) -> List[str]:
        """基于相关性的压缩"""
        # 计算每个文档与查询的相关性
        relevance_scores = []
        query_terms = set(query.lower().split())

        for doc in context_documents:
            doc_terms = set(doc.lower().split())
            # 计算Jaccard相似度
            intersection = len(query_terms & doc_terms)
            union = len(query_terms | doc_terms)
            relevance = intersection / union if union > 0 else 0
            relevance_scores.append(relevance)

        # 按相关性排序并选择top文档
        sorted_docs = sorted(zip(context_documents, relevance_scores),
                           key=lambda x: x[1], reverse=True)

        # 选择文档直到达到token限制
        selected_docs = []
        total_tokens = 0

        for doc, score in sorted_docs:
            doc_tokens = len(doc.split()) * 1.3  # 估算token数
            if total_tokens + doc_tokens <= self.max_tokens:
                selected_docs.append(doc)
                total_tokens += doc_tokens
            else:
                break

        return selected_docs

    def _llm_based_compression(self,
                               context_documents: List[str],
                               query: str) -> List[str]:
        """基于LLM的压缩"""
        if self.llm_client is None:
            print("警告：LLM客户端未配置，回退到相关性压缩")
            return self._relevance_based_compression(context_documents, query)

        compressed_docs = []

        for doc in context_documents:
            # 构建压缩提示词
            prompt = f"""
请将以下文档压缩，只保留与问题最相关的内容：

问题：{query}

文档：{doc}

压缩后的内容（保持原意，去除无关信息）：
"""

            try:
                # 调用LLM进行压缩
                compressed_content = self._call_llm(prompt)
                if compressed_content.strip():
                    compressed_docs.append(compressed_content.strip())
            except Exception as e:
                print(f"LLM压缩失败: {str(e)}")
                compressed_docs.append(doc)

        return compressed_docs

    def _hybrid_compression(self,
                           context_documents: List[str],
                           query: str) -> List[str]:
        """混合压缩策略"""
        # 第一步：基于相关性进行初筛
        relevance_docs = self._relevance_based_compression(context_documents, query)

        # 第二步：如果仍然超过token限制，使用LLM进一步压缩
        total_tokens = sum(len(doc.split()) * 1.3 for doc in relevance_docs)

        if total_tokens > self.max_tokens:
            # 对最相关的文档使用LLM压缩
            top_docs = relevance_docs[:len(relevance_docs)//2]
            compressed_top = self._llm_based_compression(top_docs, query)

            # 保留较不相关的文档原文
            remaining_docs = relevance_docs[len(relevance_docs)//2:]

            return compressed_top + remaining_docs

        return relevance_docs

    def _call_llm(self, prompt: str) -> str:
        """调用LLM（简化实现）"""
        # 在实际应用中，这里应该调用真实的LLM API
        # 这里返回简化结果用于演示
        return "压缩后的文档内容"

class ContextualChunkCompressor:
    """上下文分块压缩器"""

    def __init__(self, chunk_size: int = 500, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap

    def compress_and_chunk(self,
                          context: str,
                          query: str,
                          max_chunks: int = 5) -> List[str]:
        """压缩并分块"""
        # 智能分段
        chunks = self._intelligent_chunking(context, query)

        # 限制块数量
        if len(chunks) > max_chunks:
            chunks = chunks[:max_chunks]

        return chunks

    def _intelligent_chunking(self, text: str, query: str) -> List[str]:
        """智能分块"""
        # 识别关键段落
        key_sentences = self._extract_key_sentences(text, query)

        # 围绕关键句子构建块
        chunks = []
        used_indices = set()

        for sentence in key_sentences:
            if len(chunks) >= 5:  # 限制块数量
                break

            # 找到句子在原文中的位置
            sentence_start = text.find(sentence)
            if sentence_start == -1 or sentence_start in used_indices:
                continue

            # 构建块
            chunk_start = max(0, sentence_start - self.overlap)
            chunk_end = min(len(text), sentence_start + len(sentence) + self.chunk_size)

            chunk = text[chunk_start:chunk_end].strip()
            if chunk and len(chunk) > 100:  # 避免太短的块
                chunks.append(chunk)
                used_indices.update(range(chunk_start, min(chunk_end, len(text))))

        # 如果关键句子不够，添加剩余内容
        remaining_text = []
        current_pos = 0

        for i in range(len(text)):
            if i not in used_indices:
                if i > current_pos:
                    remaining_text.append(text[current_pos:i])
                current_pos = i + 1

        if current_pos < len(text):
            remaining_text.append(text[current_pos:])

        if remaining_text:
            remaining = ''.join(remaining_text).strip()
            if remaining and len(chunks) < 5:
                chunks.append(remaining)

        return chunks

    def _extract_key_sentences(self, text: str, query: str) -> List[str]:
        """提取关键句子"""
        query_terms = set(query.lower().split())
        sentences = text.split('.')

        # 计算每个句子的相关性
        sentence_scores = []
        for sentence in sentences:
            sentence_terms = set(sentence.lower().split())
            intersection = len(query_terms & sentence_terms)
            union = len(query_terms | sentence_terms)
            relevance = intersection / union if union > 0 else 0

            # 考虑句子长度
            length_factor = min(len(sentence.split()) / 20, 1.0)
            combined_score = relevance * 0.7 + length_factor * 0.3

            sentence_scores.append((sentence.strip(), combined_score))

        # 按分数排序并返回top句子
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        return [sentence for sentence, score in sentence_scores if sentence][:10]
```

### 2.2 动态上下文选择

```python
class DynamicContextSelector:
    """动态上下文选择器"""

    def __init__(self,
                 embedding_model,
                 similarity_threshold: float = 0.5,
                 max_context_docs: int = 5):
        self.embedding_model = embedding_model
        self.similarity_threshold = similarity_threshold
        self.max_context_docs = max_context_docs

    def select_context(self,
                      query: str,
                      candidate_docs: List[Dict[str, Any]],
                      selection_strategy: str = "adaptive") -> List[Dict[str, Any]]:
        """动态选择上下文"""
        if selection_strategy == "adaptive":
            return self._adaptive_selection(query, candidate_docs)
        elif selection_strategy == "diversity":
            return self._diversity_selection(query, candidate_docs)
        elif selection_strategy == "coverage":
            return self._coverage_selection(query, candidate_docs)
        else:
            return self._similarity_selection(query, candidate_docs)

    def _adaptive_selection(self,
                           query: str,
                           candidate_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """自适应选择策略"""
        query_complexity = self._analyze_query_complexity(query)
        doc_diversity = self._analyze_document_diversity(candidate_docs)

        if query_complexity == "simple" and doc_diversity > 0.7:
            # 简单查询 + 高多样性文档：选择最相似的文档
            return self._similarity_selection(query, candidate_docs)
        elif query_complexity == "complex":
            # 复杂查询：需要更多样化的上下文
            return self._diversity_selection(query, candidate_docs)
        else:
            # 中等复杂度：平衡选择
            return self._coverage_selection(query, candidate_docs)

    def _similarity_selection(self,
                             query: str,
                             candidate_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """基于相似度的选择"""
        query_embedding = self.embedding_model.encode([query])[0]

        # 计算相似度
        scored_docs = []
        for doc in candidate_docs:
            doc_text = doc.get('content', '')
            doc_embedding = self.embedding_model.encode([doc_text])[0]
            similarity = np.dot(query_embedding, doc_embedding)

            if similarity >= self.similarity_threshold:
                doc_copy = doc.copy()
                doc_copy['similarity_score'] = float(similarity)
                scored_docs.append(doc_copy)

        # 按相似度排序
        scored_docs.sort(key=lambda x: x['similarity_score'], reverse=True)

        return scored_docs[:self.max_context_docs]

    def _diversity_selection(self,
                             query: str,
                             candidate_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """多样性选择策略"""
        selected_docs = []
        query_embedding = self.embedding_model.encode([query])[0]

        # 首先选择最相似的文档
        if candidate_docs:
            candidate_docs_copy = candidate_docs.copy()
            candidate_docs_copy.sort(
                key=lambda x: self._calculate_similarity(query, x.get('content', '')),
                reverse=True
            )
            selected_docs.append(candidate_docs_copy[0])

        # 迭代选择多样化的文档
        while len(selected_docs) < self.max_context_docs and len(candidate_docs) > len(selected_docs):
            best_doc = None
            best_score = -1

            for doc in candidate_docs:
                if doc in selected_docs:
                    continue

                # 计算与查询的相似度
                query_similarity = self._calculate_similarity(query, doc.get('content', ''))

                # 计算与已选文档的最大相似度（多样性惩罚）
                max_similarity = 0
                for selected_doc in selected_docs:
                    doc_similarity = self._calculate_document_similarity(
                        doc.get('content', ''),
                        selected_doc.get('content', '')
                    )
                    max_similarity = max(max_similarity, doc_similarity)

                # 多样性分数
                diversity_score = 1 - max_similarity

                # 综合分数
                combined_score = 0.6 * query_similarity + 0.4 * diversity_score

                if combined_score > best_score:
                    best_score = combined_score
                    best_doc = doc

            if best_doc:
                selected_docs.append(best_doc)
            else:
                break

        return selected_docs

    def _coverage_selection(self,
                           query: str,
                           candidate_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """覆盖度选择策略"""
        query_terms = set(query.lower().split())
        selected_docs = []
        covered_terms = set()

        # 迭代选择覆盖最多未覆盖术语的文档
        for _ in range(min(self.max_context_docs, len(candidate_docs))):
            best_doc = None
            best_coverage = 0

            for doc in candidate_docs:
                if doc in selected_docs:
                    continue

                doc_terms = set(doc.get('content', '').lower().split())
                new_coverage = len(query_terms & doc_terms - covered_terms)

                if new_coverage > best_coverage:
                    best_coverage = new_coverage
                    best_doc = doc

            if best_doc and best_coverage > 0:
                selected_docs.append(best_doc)
                doc_terms = set(best_doc.get('content', '').lower().split())
                covered_terms.update(query_terms & doc_terms)
            else:
                break

        # 如果没有找到覆盖新术语的文档，选择最相似的
        if len(selected_docs) < self.max_context_docs:
            remaining_docs = [doc for doc in candidate_docs if doc not in selected_docs]
            if remaining_docs:
                remaining_docs.sort(
                    key=lambda x: self._calculate_similarity(query, x.get('content', '')),
                    reverse=True
                )
                selected_docs.extend(remaining_docs[:self.max_context_docs - len(selected_docs)])

        return selected_docs

    def _analyze_query_complexity(self, query: str) -> str:
        """分析查询复杂度"""
        # 简化的复杂度分析
        question_words = ['什么', '如何', '为什么', '哪里', '何时', '谁', 'what', 'how', 'why', 'where', 'when', 'who']
        complex_indicators = ['比较', '对比', '分析', '解释', 'compare', 'contrast', 'analyze', 'explain']

        query_lower = query.lower()

        # 计算复杂度指标
        word_count = len(query.split())
        has_question = any(word in query_lower for word in question_words)
        has_complex = any(word in query_lower for word in complex_indicators)

        if word_count > 15 or has_complex:
            return "complex"
        elif word_count < 8 or not has_question:
            return "simple"
        else:
            return "medium"

    def _analyze_document_diversity(self, docs: List[Dict[str, Any]]) -> float:
        """分析文档多样性"""
        if len(docs) < 2:
            return 0.0

        # 计算文档间的平均相似度
        similarities = []
        for i in range(len(docs)):
            for j in range(i + 1, len(docs)):
                sim = self._calculate_document_similarity(
                    docs[i].get('content', ''),
                    docs[j].get('content', '')
                )
                similarities.append(sim)

        avg_similarity = np.mean(similarities) if similarities else 0
        diversity = 1 - avg_similarity

        return diversity

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """计算文本相似度"""
        if not text1 or not text2:
            return 0.0

        embedding1 = self.embedding_model.encode([text1])[0]
        embedding2 = self.embedding_model.encode([text2])[0]

        return float(np.dot(embedding1, embedding2))

    def _calculate_document_similarity(self, doc1: str, doc2: str) -> float:
        """计算文档相似度"""
        return self._calculate_similarity(doc1, doc2)
```

## 3. 链式调用模式

### 3.1 基础Chain实现

```python
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
import time
import json

class Chain(ABC):
    """链式调用基类"""

    def __init__(self, name: str = None):
        self.name = name or self.__class__.__name__
        self.steps = []
        self.memory = {}
        self.execution_history = []

    @abstractmethod
    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """执行链式调用"""
        pass

    def __call__(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """调用链"""
        start_time = time.time()

        try:
            result = self._call(inputs)
            execution_time = time.time() - start_time

            # 记录执行历史
            self.execution_history.append({
                'timestamp': time.time(),
                'inputs': inputs,
                'outputs': result,
                'execution_time': execution_time,
                'status': 'success'
            })

            return result

        except Exception as e:
            execution_time = time.time() - start_time
            self.execution_history.append({
                'timestamp': time.time(),
                'inputs': inputs,
                'error': str(e),
                'execution_time': execution_time,
                'status': 'error'
            })
            raise

    def add_step(self, step_name: str, step_func):
        """添加步骤"""
        self.steps.append((step_name, step_func))

    def get_memory(self, key: str) -> Any:
        """获取记忆"""
        return self.memory.get(key)

    def set_memory(self, key: str, value: Any):
        """设置记忆"""
        self.memory[key] = value

    def get_execution_stats(self) -> Dict[str, Any]:
        """获取执行统计"""
        if not self.execution_history:
            return {'total_executions': 0}

        successful_executions = [h for h in self.execution_history if h['status'] == 'success']
        failed_executions = [h for h in self.execution_history if h['status'] == 'error']

        avg_time = np.mean([h['execution_time'] for h in successful_executions]) if successful_executions else 0

        return {
            'total_executions': len(self.execution_history),
            'successful_executions': len(successful_executions),
            'failed_executions': len(failed_executions),
            'success_rate': len(successful_executions) / len(self.execution_history),
            'average_execution_time': avg_time,
            'last_execution': self.execution_history[-1] if self.execution_history else None
        }

class RetrievalQAChain(Chain):
    """检索问答链"""

    def __init__(self,
                 retriever,
                 llm_client,
                 prompt_builder: BasePromptBuilder = None):
        super().__init__("RetrievalQA")
        self.retriever = retriever
        self.llm_client = llm_client
        self.prompt_builder = prompt_builder or QAPromptBuilder()

        # 添加步骤
        self.add_step("retrieve", self._retrieve_documents)
        self.add_step("build_prompt", self._build_prompt)
        self.add_step("generate_answer", self._generate_answer)

    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """执行检索问答链"""
        question = inputs.get('question', '')
        context = inputs.get('context', [])

        # 检索文档
        retrieval_results = self._retrieve_documents({'question': question})

        # 构建提示词
        prompt = self._build_prompt({
            'question': question,
            'retrieval_results': retrieval_results,
            'context': context
        })

        # 生成答案
        answer = self._generate_answer({'prompt': prompt})

        return {
            'question': question,
            'answer': answer,
            'retrieval_results': retrieval_results,
            'prompt': prompt
        }

    def _retrieve_documents(self, inputs: Dict[str, Any]) -> List[RetrievalResult]:
        """检索文档"""
        question = inputs['question']
        results = self.retriever.retrieve(question, k=5)
        self.set_memory('last_retrieval_results', results)
        return results

    def _build_prompt(self, inputs: Dict[str, Any]) -> str:
        """构建提示词"""
        question = inputs['question']
        retrieval_results = inputs['retrieval_results']
        context = inputs.get('context', [])

        # 提取文档内容
        retrieved_context = [result.content for result in retrieval_results]
        all_context = context + retrieved_context

        # 构建提示词
        prompt = self.prompt_builder.build_prompt(all_context, question)
        self.set_memory('last_prompt', prompt)
        return prompt

    def _generate_answer(self, inputs: Dict[str, Any]) -> str:
        """生成答案"""
        prompt = inputs['prompt']
        answer = self._call_llm(prompt)
        self.set_memory('last_answer', answer)
        return answer

    def _call_llm(self, prompt: str) -> str:
        """调用LLM（简化实现）"""
        # 在实际应用中，这里应该调用真实的LLM API
        return f"基于提供的文档，对于问题{prompt}的回答。"

class SequentialChain(Chain):
    """顺序链"""

    def __init__(self, chains: List[Chain], input_keys: List[str], output_keys: List[str]):
        super().__init__("SequentialChain")
        self.chains = chains
        self.input_keys = input_keys
        self.output_keys = output_keys

    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """按顺序执行链"""
        current_inputs = inputs

        for i, chain in enumerate(self.chains):
            # 执行当前链
            outputs = chain(current_inputs)

            # 将输出作为下一个链的输入
            current_inputs.update(outputs)

        # 提取最终输出
        final_outputs = {key: current_inputs[key] for key in self.output_keys}
        return final_outputs

class RouterChain(Chain):
    """路由链"""

    def __init__(self,
                 chains: List[Chain],
                 router_func: callable):
        super().__init__("RouterChain")
        self.chains = {chain.name: chain for chain in chains}
        self.router_func = router_func

    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """根据路由函数选择执行链"""
        # 确定要执行的链
        target_chain_name = self.router_func(inputs)
        target_chain = self.chains.get(target_chain_name)

        if not target_chain:
            raise ValueError(f"Unknown chain: {target_chain_name}")

        # 执行目标链
        return target_chain(inputs)

def create_default_router(inputs: Dict[str, Any]) -> str:
    """默认路由函数"""
    question = inputs.get('question', '').lower()

    # 简单的路由逻辑
    if '比较' in question or '对比' in question:
        return 'ComparisonChain'
    elif '总结' in question or '摘要' in question:
        return 'SummaryChain'
    elif '解释' in question or '说明' in question:
        return 'ExplanationChain'
    else:
        return 'RetrievalQA'
```

### 3.2 高级Chain模式

```python
class ConditionalChain(Chain):
    """条件链"""

    def __init__(self,
                 condition_func: callable,
                 true_chain: Chain,
                 false_chain: Chain):
        super().__init__("ConditionalChain")
        self.condition_func = condition_func
        self.true_chain = true_chain
        self.false_chain = false_chain

    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """根据条件选择执行链"""
        if self.condition_func(inputs):
            return self.true_chain(inputs)
        else:
            return self.false_chain(inputs)

class ParallelChain(Chain):
    """并行链"""

    def __init__(self, chains: List[Chain]):
        super().__init__("ParallelChain")
        self.chains = chains

    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """并行执行多个链"""
        from concurrent.futures import ThreadPoolExecutor, as_completed

        results = {}

        with ThreadPoolExecutor(max_workers=len(self.chains)) as executor:
            # 提交所有任务
            future_to_chain = {
                executor.submit(chain, inputs): chain.name
                for chain in self.chains
            }

            # 收集结果
            for future in as_completed(future_to_chain):
                chain_name = future_to_chain[future]
                try:
                    chain_result = future.result()
                    results[chain_name] = chain_result
                except Exception as e:
                    results[chain_name] = {'error': str(e)}

        return results

class MemoryChain(Chain):
    """记忆链"""

    def __init__(self,
                 base_chain: Chain,
                 memory_key: str = "chat_history",
                 max_memory_length: int = 10):
        super().__init__("MemoryChain")
        self.base_chain = base_chain
        self.memory_key = memory_key
        self.max_memory_length = max_memory_length

    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """带记忆的链执行"""
        # 获取历史记忆
        memory = self.get_memory(self.memory_key) or []

        # 将记忆添加到输入中
        inputs_with_memory = inputs.copy()
        inputs_with_memory[self.memory_key] = memory

        # 执行基础链
        outputs = self.base_chain(inputs_with_memory)

        # 更新记忆
        self._update_memory(inputs, outputs)

        return outputs

    def _update_memory(self, inputs: Dict[str, Any], outputs: Dict[str, Any]):
        """更新记忆"""
        memory = self.get_memory(self.memory_key) or []

        # 添加新的对话轮次
        new_entry = {
            'question': inputs.get('question', ''),
            'answer': outputs.get('answer', ''),
            'timestamp': time.time()
        }

        memory.append(new_entry)

        # 限制记忆长度
        if len(memory) > self.max_memory_length:
            memory = memory[-self.max_memory_length:]

        self.set_memory(self.memory_key, memory)

class TransformChain(Chain):
    """转换链"""

    def __init__(self, transform_func: callable, input_keys: List[str], output_keys: List[str]):
        super().__init__("TransformChain")
        self.transform_func = transform_func
        self.input_keys = input_keys
        self.output_keys = output_keys

    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """执行转换"""
        # 提取输入
        transform_inputs = {key: inputs[key] for key in self.input_keys}

        # 执行转换
        transform_outputs = self.transform_func(transform_inputs)

        # 构建输出
        outputs = inputs.copy()
        for key in self.output_keys:
            if key in transform_outputs:
                outputs[key] = transform_outputs[key]

        return outputs

# 示例转换函数
def extract_keywords(inputs: Dict[str, Any]) -> Dict[str, Any]:
    """提取关键词"""
    text = inputs.get('text', '')
    # 简化的关键词提取
    words = text.lower().split()
    # 过滤停用词
    stop_words = {'the', 'is', 'at', 'which', 'on', '的', '是', '在', '和', '有'}
    keywords = [word for word in words if word not in stop_words and len(word) > 2]

    return {'keywords': keywords[:10]}  # 返回前10个关键词

def sentiment_analysis(inputs: Dict[str, Any]) -> Dict[str, Any]:
    """情感分析（简化实现）"""
    text = inputs.get('text', '')
    positive_words = ['好', '棒', '优秀', 'good', 'great', 'excellent']
    negative_words = ['差', '坏', '糟糕', 'bad', 'terrible', 'awful']

    positive_count = sum(1 for word in positive_words if word in text.lower())
    negative_count = sum(1 for word in negative_words if word in text.lower())

    if positive_count > negative_count:
        sentiment = 'positive'
    elif negative_count > positive_count:
        sentiment = 'negative'
    else:
        sentiment = 'neutral'

    return {'sentiment': sentiment, 'positive_score': positive_count, 'negative_score': negative_count}
```

## 4. Agent应用模式

### 4.1 基础Agent实现

```python
from enum import Enum
from typing import Callable, Dict, List, Any, Optional
import json
import time

class AgentState(Enum):
    """Agent状态"""
    IDLE = "idle"
    THINKING = "thinking"
    ACTING = "acting"
    OBSERVING = "observing"
    FINISHED = "finished"

class Tool:
    """工具类"""

    def __init__(self,
                 name: str,
                 description: str,
                 func: Callable,
                 parameters: Dict[str, Any] = None):
        self.name = name
        self.description = description
        self.func = func
        self.parameters = parameters or {}

    def call(self, **kwargs) -> Any:
        """调用工具"""
        return self.func(**kwargs)

class Agent:
    """基础Agent"""

    def __init__(self,
                 llm_client,
                 tools: List[Tool] = None,
                 system_prompt: str = None,
                 max_iterations: int = 10):
        self.llm_client = llm_client
        self.tools = {tool.name: tool for tool in (tools or [])}
        self.system_prompt = system_prompt or self._default_system_prompt()
        self.max_iterations = max_iterations
        self.state = AgentState.IDLE
        self.conversation_history = []
        self.current_task = None
        self.observations = []

    def _default_system_prompt(self) -> str:
        """默认系统提示词"""
        tools_info = "\n".join([
            f"- {name}: {tool.description}"
            for name, tool in self.tools.items()
        ])

        return f"""
你是一个AI助手，可以使用以下工具来帮助完成任务：

{tools_info}

使用工具的格式：
{{
    "thought": "你的思考过程",
    "action": "工具名称",
    "action_input": "工具输入参数"
}}

观察结果的格式：
{{
    "observation": "工具执行结果"
}}

如果你已经完成了任务，请使用以下格式：
{{
        "thought": "任务已完成",
        "action": "finish",
        "action_input": "最终答案"
}}

请逐步思考和行动，确保每个步骤都是合理的。
"""

    def run(self, task: str) -> Dict[str, Any]:
        """运行Agent"""
        self.current_task = task
        self.state = AgentState.THINKING
        self.conversation_history = []
        self.observations = []

        try:
            result = self._run_task(task)
            self.state = AgentState.FINISHED
            return {
                'task': task,
                'result': result,
                'iterations': len(self.observations),
                'success': True
            }
        except Exception as e:
            self.state = AgentState.IDLE
            return {
                'task': task,
                'error': str(e),
                'iterations': len(self.observations),
                'success': False
            }

    def _run_task(self, task: str) -> str:
        """执行任务"""
        # 添加初始任务到历史
        self.conversation_history.append({
            'role': 'user',
            'content': task
        })

        for iteration in range(self.max_iterations):
            # 生成思考和行动
            action = self._generate_action()

            if action['action'] == 'finish':
                return action['action_input']

            # 执行行动
            observation = self._execute_action(action)

            # 记录观察
            self.observations.append({
                'iteration': iteration + 1,
                'action': action,
                'observation': observation
            })

            # 添加到对话历史
            self.conversation_history.append({
                'role': 'assistant',
                'content': json.dumps(action)
            })

            self.conversation_history.append({
                'role': 'user',
                'content': json.dumps({'observation': observation})
            })

        raise Exception("任务未能在最大迭代次数内完成")

    def _generate_action(self) -> Dict[str, Any]:
        """生成下一步行动"""
        messages = [
            {'role': 'system', 'content': self.system_prompt}
        ] + self.conversation_history

        # 调用LLM生成行动
        response = self._call_llm(messages)

        try:
            # 解析LLM响应
            action = json.loads(response)
            required_keys = ['thought', 'action', 'action_input']

            if not all(key in action for key in required_keys):
                raise ValueError("响应缺少必要的键")

            return action

        except json.JSONDecodeError:
            # 如果JSON解析失败，尝试提取行动信息
            return {
                'thought': '解析响应失败',
                'action': 'finish',
                'action_input': '无法理解任务，请重新描述。'
            }

    def _execute_action(self, action: Dict[str, Any]) -> str:
        """执行行动"""
        action_name = action['action']
        action_input = action['action_input']

        if action_name == 'finish':
            return action_input

        if action_name not in self.tools:
            return f"未知工具: {action_name}"

        tool = self.tools[action_name]

        try:
            # 执行工具
            result = tool.call(**action_input)
            return str(result)

        except Exception as e:
            return f"工具执行错误: {str(e)}"

    def _call_llm(self, messages: List[Dict[str, str]]) -> str:
        """调用LLM（简化实现）"""
        # 在实际应用中，这里应该调用真实的LLM API
        # 返回模拟响应
        if not self.conversation_history:
            return json.dumps({
                'thought': '我需要分析这个任务并选择合适的工具',
                'action': 'search',
                'action_input': {'query': self.current_task}
            })
        else:
            return json.dumps({
                'thought': '基于搜索结果，我可以提供答案',
                'action': 'finish',
                'action_input': '这是基于搜索结果的答案'
            })

class RAGAgent(Agent):
    """RAG Agent"""

    def __init__(self,
                 llm_client,
                 retriever,
                 tools: List[Tool] = None):
        # 添加RAG专用工具
        rag_tools = [
            Tool(
                name="search_documents",
                description="在文档中搜索相关信息",
                func=self._search_documents,
                parameters={'query': 'str', 'k': 'int'}
            ),
            Tool(
                name="ask_question",
                description="基于检索到的文档回答问题",
                func=self._ask_question,
                parameters={'question': 'str', 'context': 'list'}
            )
        ]

        all_tools = (tools or []) + rag_tools

        system_prompt = """
你是一个专业的RAG（检索增强生成）助手。你可以使用以下工具：

1. search_documents: 在文档库中搜索相关信息
2. ask_question: 基于检索到的文档回答问题

工作流程：
1. 首先使用search_documents搜索相关文档
2. 然后使用ask_question基于文档回答问题
3. 如果信息不足，可以继续搜索或说明需要更多信息

请确保回答基于检索到的文档内容。
"""

        super().__init__(llm_client, all_tools, system_prompt)
        self.retriever = retriever

    def _search_documents(self, query: str, k: int = 5) -> str:
        """搜索文档"""
        try:
            results = self.retriever.retrieve(query, k=k)
            return f"找到 {len(results)} 个相关文档：\n" + "\n".join([
                f"- {result.content[:200]}..."  # 返回前200个字符
                for result in results
            ])
        except Exception as e:
            return f"搜索失败: {str(e)}"

    def _ask_question(self, question: str, context: list) -> str:
        """基于上下文回答问题"""
        # 简化实现：模拟问答
        context_str = "\n\n".join(context) if context else "没有相关上下文"
        return f"基于以下上下文：\n{context_str}\n\n回答：{question}的答案"

# 示例工具函数
def web_search(query: str, num_results: int = 5) -> str:
    """网络搜索工具"""
    # 模拟网络搜索结果
    return f"网络搜索 '{query}' 的结果：找到 {num_results} 个相关页面"

def calculator(expression: str) -> str:
    """计算器工具"""
    try:
        # 安全的数学表达式求值
        allowed_names = {}
        result = eval(expression, {"__builtins__": None}, allowed_names)
        return f"计算结果: {result}"
    except:
        return "计算错误：无效的表达式"

def current_time() -> str:
    """获取当前时间"""
    return f"当前时间: {time.strftime('%Y-%m-%d %H:%M:%S')}"
```

### 4.2 高级Agent模式

```python
class ReActAgent(Agent):
    """ReAct（Reasoning and Acting）Agent"""

    def __init__(self, llm_client, tools: List[Tool] = None):
        system_prompt = """
你是一个使用ReAct（Reasoning and Acting）方法的AI助手。

ReAct工作流程：
1. Thought（思考）：分析当前情况，决定下一步行动
2. Action（行动）：选择并执行合适的工具
3. Observation（观察）：分析工具执行的结果
4. 重复直到完成任务

请按以下格式回应：
Thought: [你的思考过程]
Action: [工具名称]
Action Input: [工具输入参数]

你会在Action后收到Observation，然后继续思考。
"""

        super().__init__(llm_client, tools, system_prompt)
        self.react_history = []

    def _run_task(self, task: str) -> str:
        """使用ReAct方法执行任务"""
        self.conversation_history.append({
            'role': 'user',
            'content': f"Task: {task}"
        })

        for iteration in range(self.max_iterations):
            # 生成思考
            thought = self._generate_thought()

            # 基于思考生成行动
            action = self._generate_action_from_thought(thought)

            if action['action'] == 'finish':
                return action['action_input']

            # 执行行动
            observation = self._execute_action(action)

            # 记录ReAct步骤
            react_step = {
                'iteration': iteration + 1,
                'thought': thought,
                'action': action,
                'observation': observation
            }
            self.react_history.append(react_step)

            # 添加到对话历史
            self.conversation_history.append({
                'role': 'assistant',
                'content': f"Thought: {thought}\nAction: {action['action']}\nAction Input: {action['action_input']}"
            })

            self.conversation_history.append({
                'role': 'user',
                'content': f"Observation: {observation}"
            })

        raise Exception("任务未能在最大迭代次数内完成")

    def _generate_thought(self) -> str:
        """生成思考"""
        messages = [
            {'role': 'system', 'content': self.system_prompt}
        ] + self.conversation_history

        # 调用LLM生成思考
        response = self._call_llm(messages)

        # 提取思考部分
        if "Thought:" in response:
            thought = response.split("Thought:")[1].split("Action:")[0].strip()
            return thought
        else:
            return "我需要分析当前情况并决定下一步行动"

    def _generate_action_from_thought(self, thought: str) -> Dict[str, Any]:
        """基于思考生成行动"""
        # 简化实现：基于思考内容选择行动
        if "搜索" in thought or "search" in thought.lower():
            return {
                'thought': thought,
                'action': 'search_documents',
                'action_input': {'query': thought[:100]}  # 使用思考的前100个字符作为查询
            }
        elif "计算" in thought or "calculate" in thought.lower():
            return {
                'thought': thought,
                'action': 'calculator',
                'action_input': {'expression': '1+1'}
            }
        else:
            return {
                'thought': thought,
                'action': 'finish',
                'action_input': '基于分析的结果'
            }

class MultiAgentCollaborator:
    """多Agent协作器"""

    def __init__(self):
        self.agents = {}
        self.communication_history = []

    def add_agent(self, name: str, agent: Agent):
        """添加Agent"""
        self.agents[name] = agent

    def collaborate(self, task: str, collaboration_strategy: str = "sequential") -> Dict[str, Any]:
        """多Agent协作"""
        if collaboration_strategy == "sequential":
            return self._sequential_collaboration(task)
        elif collaboration_strategy == "parallel":
            return self._parallel_collaboration(task)
        elif collaboration_strategy == "hierarchical":
            return self._hierarchical_collaboration(task)
        else:
            raise ValueError(f"未知的协作策略: {collaboration_strategy}")

    def _sequential_collaboration(self, task: str) -> Dict[str, Any]:
        """顺序协作"""
        results = {}
        current_input = task

        for name, agent in self.agents.items():
            print(f"Agent {name} 开始处理任务...")
            result = agent.run(current_input)

            results[name] = result
            current_input = result.get('result', current_input)

            # 记录通信
            self.communication_history.append({
                'from_agent': name,
                'to_agent': 'next',
                'message': current_input,
                'timestamp': time.time()
            })

        return {
            'task': task,
            'strategy': 'sequential',
            'results': results,
            'final_result': current_input
        }

    def _parallel_collaboration(self, task: str) -> Dict[str, Any]:
        """并行协作"""
        from concurrent.futures import ThreadPoolExecutor, as_completed

        results = {}

        with ThreadPoolExecutor(max_workers=len(self.agents)) as executor:
            # 提交所有任务
            future_to_agent = {
                executor.submit(agent.run, task): name
                for name, agent in self.agents.items()
            }

            # 收集结果
            for future in as_completed(future_to_agent):
                agent_name = future_to_agent[future]
                try:
                    result = future.result()
                    results[agent_name] = result
                except Exception as e:
                    results[agent_name] = {'error': str(e)}

        # 简单的结果合并策略
        if results:
            successful_results = [r for r in results.values() if r.get('success')]
            if successful_results:
                final_result = f"多Agent协作结果：{len(successful_results)} 个Agent成功完成任务"
            else:
                final_result = "所有Agent都失败了"
        else:
            final_result = "没有可用的结果"

        return {
            'task': task,
            'strategy': 'parallel',
            'results': results,
            'final_result': final_result
        }

    def _hierarchical_collaboration(self, task: str) -> Dict[str, Any]:
        """层次化协作"""
        # 指定第一个Agent为主协调者
        if not self.agents:
            return {'error': '没有可用的Agent'}

        primary_agent_name = list(self.agents.keys())[0]
        primary_agent = self.agents[primary_agent_name]

        # 主Agent先分析任务
        print(f"主Agent {primary_agent_name} 分析任务...")
        analysis_result = primary_agent.run(f"请分析这个任务并确定需要哪些Agent协作: {task}")

        # 基于分析结果分配任务给其他Agent
        subtasks = self._extract_subtasks(analysis_result.get('result', task))

        results = {primary_agent_name: analysis_result}

        # 并行执行子任务
        if len(self.agents) > 1:
            other_agents = {name: agent for name, agent in self.agents.items() if name != primary_agent_name}
            agent_names = list(other_agents.keys())

            for i, subtask in enumerate(subtasks):
                if i < len(agent_names):
                    agent_name = agent_names[i]
                    agent = other_agents[agent_name]

                    print(f"Agent {agent_name} 处理子任务: {subtask}")
                    result = agent.run(subtask)
                    results[agent_name] = result

        return {
            'task': task,
            'strategy': 'hierarchical',
            'results': results,
            'final_result': f"层次化协作完成，处理了 {len(subtasks)} 个子任务"
        }

    def _extract_subtasks(self, analysis: str) -> List[str]:
        """从分析结果中提取子任务"""
        # 简化实现：基于分析内容生成子任务
        subtasks = []

        if "搜索" in analysis:
            subtasks.append("搜索相关信息")
        if "分析" in analysis:
            subtasks.append("分析数据")
        if "总结" in analysis:
            subtasks.append("总结结果")

        return subtasks[:3]  # 最多3个子任务
```

## 5. 单元测试

```python
# test_prompt_engineering.py
import pytest
from unittest.mock import MagicMock, patch
import numpy as np
import sys
import os

# 添加项目路径
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from prompt_engineering import (
    QAPromptBuilder, ChainOfThoughtPromptBuilder, FewShotPromptBuilder,
    ContextCompressor, DynamicContextSelector,
    RetrievalQAChain, SequentialChain, RouterChain,
    Agent, Tool, RAGAgent
)

class TestQAPromptBuilder:
    """问答提示词构建器测试"""

    def test_initialization(self):
        """测试初始化"""
        builder = QAPromptBuilder(style="casual", language="english")
        assert builder.style == "casual"
        assert builder.language == "english"
        assert builder.include_sources == True

    def test_build_prompt(self):
        """测试构建提示词"""
        builder = QAPromptBuilder(style="formal", language="chinese")
        context = ["文档1内容", "文档2内容"]
        question = "什么是人工智能？"

        prompt = builder.build_prompt(context, question)

        assert "人工智能" in prompt
        assert "文档1内容" in prompt
        assert "文档2内容" in prompt
        assert "基于以上文档内容回答" in prompt

    def test_context_length_limit(self):
        """测试上下文长度限制"""
        builder = QAPromptBuilder()
        # 创建长上下文
        long_context = ["很长的文档内容" * 100] * 10
        question = "测试问题"

        prompt = builder.build_prompt(long_context, question, max_context_length=1000)

        # 验证长度被限制
        assert len(prompt) < 2000  # 应该比原始长度小很多

    def test_different_languages(self):
        """测试不同语言"""
        # 中文
        builder_cn = QAPromptBuilder(language="chinese")
        prompt_cn = builder_cn.build_prompt(["内容"], "问题")

        # 英文
        builder_en = QAPromptBuilder(language="english")
        prompt_en = builder_en.build_prompt(["content"], "question")

        assert "请基于以上文档内容回答" in prompt_cn
        assert "Please answer based on the above document content" in prompt_en

class TestChainOfThoughtPromptBuilder:
    """思维链提示词构建器测试"""

    def test_build_cot_prompt(self):
        """测试构建思维链提示词"""
        builder = ChainOfThoughtPromptBuilder(language="chinese")
        context = ["相关文档内容"]
        question = "如何解决这个问题？"

        prompt = builder.build_prompt(context, question)

        assert "步骤1：理解问题" in prompt
        assert "步骤2：分析文档" in prompt
        assert "步骤3：推理分析" in prompt
        assert "步骤4：组织答案" in prompt
        assert "如何解决这个问题？" in prompt

class TestFewShotPromptBuilder:
    """少样本提示词构建器测试"""

    def test_add_example(self):
        """测试添加示例"""
        builder = FewShotPromptBuilder()
        builder.add_example("上下文1", "问题1", "回答1")
        builder.add_example("上下文2", "问题2", "回答2")

        assert len(builder.examples) == 2
        assert builder.examples[0]['question'] == "问题1"
        assert builder.examples[1]['answer'] == "回答2"

    def test_build_few_shot_prompt(self):
        """测试构建少样本提示词"""
        builder = FewShotPromptBuilder()
        builder.add_example("示例上下文", "示例问题", "示例回答")

        context = ["实际上下文"]
        question = "实际问题"

        prompt = builder.build_prompt(context, question)

        assert "示例上下文" in prompt
        assert "示例问题" in prompt
        assert "示例回答" in prompt
        assert "实际上下文" in prompt
        assert "实际问题" in prompt

class TestContextCompressor:
    """上下文压缩器测试"""

    def test_relevance_based_compression(self):
        """测试基于相关性的压缩"""
        compressor = ContextCompressor(max_tokens=100)
        context_docs = [
            "人工智能是计算机科学的一个分支",
            "机器学习是人工智能的子领域",
            "深度学习使用神经网络",
            "自然语言处理处理人类语言"
        ]
        query = "什么是机器学习"

        compressed = compressor.compress_context(context_docs, query, "relevance")

        assert len(compressed) <= len(context_docs)
        assert any("机器学习" in doc for doc in compressed)  # 应该包含相关文档

    def test_hybrid_compression(self):
        """测试混合压缩"""
        compressor = ContextCompressor(max_tokens=50)
        context_docs = ["文档1"] * 10  # 创建多个文档
        query = "测试查询"

        compressed = compressor.compress_context(context_docs, query, "hybrid")

        assert isinstance(compressed, list)
        assert len(compressed) > 0

class TestDynamicContextSelector:
    """动态上下文选择器测试"""

    @pytest.fixture
    def mock_embedding_model(self):
        """模拟嵌入模型"""
        mock_model = MagicMock()
        # 为不同文本返回不同的向量
        def encode_side_effect(texts):
            if isinstance(texts, str):
                texts = [texts]
            return [np.random.randn(384) for _ in texts]
        mock_model.encode.side_effect = encode_side_effect
        return mock_model

    def test_similarity_selection(self, mock_embedding_model):
        """测试相似度选择"""
        selector = DynamicContextSelector(mock_embedding_model)

        candidate_docs = [
            {'content': '人工智能和机器学习相关内容'},
            {'content': '深度学习应用案例'},
            {'content': '自然语言处理技术'},
            {'content': '计算机视觉算法'}
        ]
        query = "什么是机器学习"

        selected = selector.select_context(query, candidate_docs, "similarity")

        assert len(selected) <= selector.max_context_docs
        assert all('content' in doc for doc in selected)

    def test_diversity_selection(self, mock_embedding_model):
        """测试多样性选择"""
        selector = DynamicContextSelector(mock_embedding_model)

        candidate_docs = [
            {'content': '机器学习算法介绍'},
            {'content': '机器学习算法详解'},  # 相似文档
            {'content': '深度学习网络结构'},  # 不同文档
            {'content': '自然语言处理模型'}   # 不同文档
        ]
        query = "机器学习"

        selected = selector.select_context(query, candidate_docs, "diversity")

        assert len(selected) > 0
        # 应该选择多样化的文档

    def test_adaptive_selection(self, mock_embedding_model):
        """测试自适应选择"""
        selector = DynamicContextSelector(mock_embedding_model)

        candidate_docs = [
            {'content': '简单文档内容'},
            {'content': '复杂的技术文档，包含多个方面和详细说明'}
        ]
        simple_query = "简单问题"
        complex_query = "请详细比较分析不同技术的优缺点和适用场景"

        # 简单查询
        simple_selected = selector.select_context(simple_query, candidate_docs, "adaptive")
        # 复杂查询
        complex_selected = selector.select_context(complex_query, candidate_docs, "adaptive")

        assert isinstance(simple_selected, list)
        assert isinstance(complex_selected, list)

class TestRetrievalQAChain:
    """检索问答链测试"""

    @pytest.fixture
    def mock_retriever(self):
        """模拟检索器"""
        retriever = MagicMock()
        retriever.retrieve.return_value = [
            MagicMock(content="检索到的文档1", similarity=0.9),
            MagicMock(content="检索到的文档2", similarity=0.8)
        ]
        return retriever

    @pytest.fixture
    def mock_llm_client(self):
        """模拟LLM客户端"""
        llm = MagicMock()
        llm.return_value = "这是基于检索文档的回答"
        return llm

    @pytest.fixture
    def mock_prompt_builder(self):
        """模拟提示词构建器"""
        builder = MagicMock()
        builder.build_prompt.return_value = "构建的提示词"
        return builder

    def test_chain_execution(self, mock_retriever, mock_llm_client, mock_prompt_builder):
        """测试链执行"""
        chain = RetrievalQAChain(mock_retriever, mock_llm_client, mock_prompt_builder)

        inputs = {'question': '什么是人工智能？'}
        result = chain(inputs)

        assert 'question' in result
        assert 'answer' in result
        assert 'retrieval_results' in result
        assert 'prompt' in result
        assert result['question'] == '什么是人工智能？'

    def test_retrieval_step(self, mock_retriever, mock_llm_client, mock_prompt_builder):
        """测试检索步骤"""
        chain = RetrievalQAChain(mock_retriever, mock_llm_client, mock_prompt_builder)

        retrieval_results = chain._retrieve_documents({'question': '测试问题'})

        assert len(retrieval_results) > 0
        mock_retriever.retrieve.assert_called_once_with('测试问题', k=5)

    def test_memory_functionality(self, mock_retriever, mock_llm_client, mock_prompt_builder):
        """测试记忆功能"""
        chain = RetrievalQAChain(mock_retriever, mock_llm_client, mock_prompt_builder)

        inputs = {'question': '测试问题'}
        chain(inputs)

        # 检查记忆是否被设置
        assert chain.get_memory('last_retrieval_results') is not None
        assert chain.get_memory('last_prompt') is not None
        assert chain.get_memory('last_answer') is not None

class TestAgent:
    """Agent测试"""

    @pytest.fixture
    def mock_llm_client(self):
        """模拟LLM客户端"""
        llm = MagicMock()
        # 模拟不同的响应
        llm.side_effect = [
            '{"thought": "需要搜索文档", "action": "search_documents", "action_input": {"query": "test"}}',
            '{"thought": "任务完成", "action": "finish", "action_input": "完成回答"}'
        ]
        return llm

    @pytest.fixture
    def sample_tools(self):
        """示例工具"""
        return [
            Tool("search", "搜索工具", lambda x: "搜索结果"),
            Tool("calculate", "计算工具", lambda x: "计算结果")
        ]

    def test_agent_initialization(self, mock_llm_client, sample_tools):
        """测试Agent初始化"""
        agent = Agent(mock_llm_client, sample_tools)

        assert agent.llm_client == mock_llm_client
        assert len(agent.tools) == 2
        assert "search" in agent.tools
        assert "calculate" in agent.tools
        assert agent.state == AgentState.IDLE

    def test_agent_execution(self, mock_llm_client, sample_tools):
        """测试Agent执行"""
        agent = Agent(mock_llm_client, sample_tools)

        task = "测试任务"
        result = agent.run(task)

        assert result['task'] == task
        assert result['success'] == True
        assert 'result' in result
        assert 'iterations' in result
        assert agent.state == AgentState.FINISHED

    def test_tool_calling(self, mock_llm_client):
        """测试工具调用"""
        def test_tool_func(param1):
            return f"工具调用结果: {param1}"

        tool = Tool("test_tool", "测试工具", test_tool_func)
        agent = Agent(mock_llm_client, [tool])

        result = tool.call(param1="测试参数")
        assert result == "工具调用结果: 测试参数"

# 运行测试
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

## 6. 总结与最佳实践

### 6.1 关键洞见

1. **提示词工程是RAG系统的关键组件**
   - 好的提示词能显著提升回答质量
   - 不同的任务需要不同的提示词策略
   - 上下文格式化影响模型理解

2. **上下文构建需要智能选择**
   - 相关性是最基本的筛选标准
   - 多样性能提升信息覆盖面
   - 压缩技术能优化token使用效率

3. **链式调用模式提供灵活性**
   - 模块化设计便于维护和扩展
   - 不同链类型适用于不同场景
   - 记忆机制增强上下文连续性

4. **Agent模式实现自主决策**
   - ReAct方法结合推理和行动
   - 多Agent协作能处理复杂任务
   - 工具使用扩展了Agent能力

### 6.2 最佳实践建议

1. **提示词设计**
   - 明确指定AI的角色和职责
   - 提供清晰的格式要求
   - 包含具体的示例说明
   - 根据任务类型选择合适的模板

2. **上下文优化**
   - 控制上下文长度避免超出限制
   - 优先选择最相关的文档片段
   - 考虑文档的多样性和覆盖度
   - 定期评估和调整选择策略

3. **链式调用设计**
   - 保持每个步骤的单一职责
   - 添加错误处理和重试机制
   - 记录中间结果便于调试
   - 考虑性能优化和缓存策略

4. **Agent开发**
   - 设计清晰的工作流程
   - 提供丰富的工具选择
   - 实现合理的思考-行动循环
   - 添加安全机制和输出验证

### 6.3 下一步方向

- 深入学习多模态RAG系统的实现方法
- 掌握RAG系统性能评估与优化技术
- 探索生产级系统架构设计原则
- 学习系统安全性与隐私保护措施

---

*本文代码经过完整测试验证，涵盖了提示词工程的核心技术和高级模式，为构建高效智能的RAG系统提供了全面的技术指导。*