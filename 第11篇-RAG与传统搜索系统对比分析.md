# ç¬¬11ç¯‡ï¼šRAGä¸ä¼ ç»Ÿæœç´¢ç³»ç»Ÿå¯¹æ¯”åˆ†æ

## æ‘˜è¦

æœ¬æ–‡æ·±å…¥å¯¹æ¯”åˆ†æäº†RAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿä¸ä¼ ç»Ÿæœç´¢ç³»ç»Ÿçš„ä¼˜ç¼ºç‚¹ï¼Œé€šè¿‡å¤§é‡å®éªŒæ•°æ®å’Œæ¡ˆä¾‹åˆ†æï¼Œæ¢è®¨äº†ä¸¤è€…çš„æŠ€æœ¯å·®å¼‚ã€é€‚ç”¨åœºæ™¯ä»¥åŠèåˆå‘å±•çš„å¯èƒ½æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒRAGå¹¶éä¼ ç»Ÿæœç´¢çš„æ›¿ä»£å“ï¼Œè€Œæ˜¯å…¶é‡è¦è¡¥å……å’Œå‡çº§ã€‚

## 1. æœç´¢æŠ€æœ¯å‘å±•å†ç¨‹

### 1.1 æœç´¢ç³»ç»Ÿæ¼”è¿›è·¯çº¿å›¾

```
æœç´¢æŠ€æœ¯å‘å±•æ—¶é—´çº¿ï¼š
â”œâ”€â”€ ç¬¬ä¸€ä»£ï¼šå…³é”®è¯æœç´¢ (1990s)
â”‚   â”œâ”€â”€ æŠ€æœ¯ç‰¹ç‚¹ï¼šåŸºäºTF-IDFçš„å€’æ’ç´¢å¼•
â”‚   â”œâ”€â”€ ä¼˜åŠ¿ï¼šç®€å•é«˜æ•ˆã€å¿«é€Ÿå“åº”
â”‚   â””â”€â”€ å±€é™ï¼šè¯­ä¹‰ç†è§£èƒ½åŠ›æœ‰é™
â”œâ”€â”€ ç¬¬äºŒä»£ï¼šé“¾æ¥åˆ†ææœç´¢ (2000s)
â”‚   â”œâ”€â”€ æŠ€æœ¯ç‰¹ç‚¹ï¼šPageRankç®—æ³•ã€é“¾æ¥æƒé‡
â”‚   â”œâ”€â”€ ä¼˜åŠ¿ï¼šç»“æœè´¨é‡æ˜¾è‘—æå‡
â”‚   â””â”€â”€ å±€é™ï¼šä»ç„¶ä¾èµ–å…³é”®è¯åŒ¹é…
â”œâ”€â”€ ç¬¬ä¸‰ä»£ï¼šè¯­ä¹‰æœç´¢ (2010s)
â”‚   â”œâ”€â”€ æŠ€æœ¯ç‰¹ç‚¹ï¼šè¯å‘é‡ã€å®ä½“è¯†åˆ«ã€çŸ¥è¯†å›¾è°±
â”‚   â”œâ”€â”€ ä¼˜åŠ¿ï¼šç†è§£æŸ¥è¯¢æ„å›¾ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥
â”‚   â””â”€â”€ å±€é™ï¼šç”Ÿæˆèƒ½åŠ›æœ‰é™
â””â”€â”€ ç¬¬å››ä»£ï¼šæ™ºèƒ½ç”Ÿæˆæœç´¢ (2020s)
    â”œâ”€â”€ æŠ€æœ¯ç‰¹ç‚¹ï¼šå¤§è¯­è¨€æ¨¡å‹ã€RAGæ¶æ„
    â”œâ”€â”€ ä¼˜åŠ¿ï¼šç›´æ¥å›ç­”ã€æ¨ç†èƒ½åŠ›
    â””â”€â”€ å±€é™ï¼šå®æ—¶æ€§ã€å‡†ç¡®æ€§æŒ‘æˆ˜
```

### 1.2 æŠ€æœ¯èŒƒå¼å¯¹æ¯”

| ç»´åº¦ | ä¼ ç»Ÿæœç´¢ | RAGæœç´¢ |
|------|----------|---------|
| **æ ¸å¿ƒåŸç†** | ç´¢å¼•åŒ¹é… + æ’åº | æ£€ç´¢å¢å¼º + ç”Ÿæˆ |
| **è¾“å‡ºå½¢å¼** | æ–‡æ¡£åˆ—è¡¨ | è‡ªç„¶è¯­è¨€å›ç­” |
| **ç†è§£æ·±åº¦** | å…³é”®è¯çº§ | è¯­ä¹‰ç†è§£çº§ |
| **æ¨ç†èƒ½åŠ›** | æ—  | å¼ºæ¨ç†èƒ½åŠ› |
| **å®æ—¶æ€§** | é«˜ | ä¸­ç­‰ |
| **å‡†ç¡®æ€§** | å¯éªŒè¯ | éœ€äº‹å®æ ¸æŸ¥ |

## 2. æŠ€æœ¯æ¶æ„æ·±åº¦å¯¹æ¯”

### 2.1 ä¼ ç»Ÿæœç´¢ç³»ç»Ÿæ¶æ„

```python
from typing import List, Dict, Tuple
import re
import math
from collections import defaultdict, Counter
import jieba

class TraditionalSearchEngine:
    """ä¼ ç»Ÿæœç´¢å¼•æ“å®ç°"""

    def __init__(self):
        self.documents = {}  # æ–‡æ¡£åº“
        self.inverted_index = defaultdict(list)  # å€’æ’ç´¢å¼•
        self.document_vectors = {}  # æ–‡æ¡£å‘é‡
        self.document_lengths = {}  # æ–‡æ¡£é•¿åº¦
        self.avg_doc_length = 0  # å¹³å‡æ–‡æ¡£é•¿åº¦

    def add_documents(self, documents: List[Dict]):
        """æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•"""
        total_length = 0

        for doc in documents:
            doc_id = doc['id']
            content = doc['content']

            # åˆ†è¯å’Œé¢„å¤„ç†
            tokens = self._tokenize(content)
            cleaned_tokens = self._clean_tokens(tokens)

            # æ„å»ºå€’æ’ç´¢å¼•
            term_counts = Counter(cleaned_tokens)
            for term, count in term_counts.items():
                self.inverted_index[term].append((doc_id, count))

            # å­˜å‚¨æ–‡æ¡£ä¿¡æ¯
            self.documents[doc_id] = {
                'id': doc_id,
                'content': content,
                'title': doc.get('title', ''),
                'tokens': cleaned_tokens,
                'term_counts': term_counts
            }

            self.document_lengths[doc_id] = len(cleaned_tokens)
            total_length += len(cleaned_tokens)

        # è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦
        if self.documents:
            self.avg_doc_length = total_length / len(self.documents)

    def _tokenize(self, text: str) -> List[str]:
        """ä¸­æ–‡åˆ†è¯"""
        return list(jieba.cut(text.lower()))

    def _clean_tokens(self, tokens: List[str]) -> List[str]:
        """æ¸…ç†åœç”¨è¯å’Œæ ‡ç‚¹"""
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'å°±æ˜¯', 'è€Œ', 'è¿˜æ˜¯', 'æ¯”', 'æ¥', 'æ—¶å€™', 'è®©', 'ä»', 'æŠŠ', 'è¢«', 'ä¸º', 'è¿™ä¸ª', 'ä»€ä¹ˆ', 'èƒ½', 'å¯ä»¥', 'å¥¹', 'ä»–', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'å—', 'å‘¢', 'å§', 'å•Š', 'å“¦', 'å“ˆ', 'å“ˆ', 'å—¯', 'å“¼', 'å˜¿', 'å–‚', 'å”‰', 'å’¦', 'å™¢'}

        cleaned = []
        for token in tokens:
            if (len(token.strip()) > 1 and
                token not in stop_words and
                not re.match(r'^[^\w\s]$', token)):
                cleaned.append(token)

        return cleaned

    def calculate_tfidf(self, query_tokens: List[str], doc_id: str) -> float:
        """è®¡ç®—TF-IDFåˆ†æ•°"""
        score = 0.0
        doc = self.documents[doc_id]
        doc_length = self.document_lengths[doc_id]
        total_docs = len(self.documents)

        for term in query_tokens:
            # TF (è¯é¢‘)
            tf = doc['term_counts'].get(term, 0)

            if tf > 0:
                # IDF (é€†æ–‡æ¡£é¢‘ç‡)
                df = len(self.inverted_index[term])
                idf = math.log((total_docs - df + 0.5) / (df + 0.5))

                # BM25 TFè®¡ç®—
                k1 = 1.2
                b = 0.75
                tf_score = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length / self.avg_doc_length)))

                score += tf_score * idf

        return score

    def search(self, query: str, top_k: int = 10) -> List[Dict]:
        """æ‰§è¡Œæœç´¢"""
        # æŸ¥è¯¢é¢„å¤„ç†
        query_tokens = self._clean_tokens(self._tokenize(query))

        if not query_tokens:
            return []

        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„ç›¸å…³æ€§åˆ†æ•°
        doc_scores = []
        for doc_id in self.documents:
            score = self.calculate_tfidf(query_tokens, doc_id)
            if score > 0:
                doc_scores.append((doc_id, score))

        # æŒ‰åˆ†æ•°æ’åº
        doc_scores.sort(key=lambda x: x[1], reverse=True)

        # è¿”å›ç»“æœ
        results = []
        for doc_id, score in doc_scores[:top_k]:
            doc = self.documents[doc_id]

            # æå–æ‘˜è¦
            snippet = self._extract_snippet(doc['content'], query_tokens)

            results.append({
                'doc_id': doc_id,
                'title': doc['title'],
                'snippet': snippet,
                'score': score,
                'url': f"/document/{doc_id}",
                'type': 'traditional_search'
            })

        return results

    def _extract_snippet(self, content: str, query_tokens: List[str], snippet_length: int = 200) -> str:
        """æå–åŒ…å«æŸ¥è¯¢è¯çš„æ‘˜è¦"""
        # ç®€åŒ–çš„æ‘˜è¦æå–é€»è¾‘
        content_lower = content.lower()
        query_lower = [token.lower() for token in query_tokens]

        # å¯»æ‰¾ç¬¬ä¸€ä¸ªåŒ¹é…çš„ä½ç½®
        best_pos = -1
        for token in query_lower:
            pos = content_lower.find(token)
            if pos != -1 and (best_pos == -1 or pos < best_pos):
                best_pos = pos

        if best_pos == -1:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…è¯ï¼Œè¿”å›å¼€å¤´
            return content[:snippet_length] + "..."

        # æå–ä»¥åŒ¹é…è¯ä¸ºä¸­å¿ƒçš„ç‰‡æ®µ
        start = max(0, best_pos - snippet_length // 2)
        end = min(len(content), start + snippet_length)

        snippet = content[start:end]
        if start > 0:
            snippet = "..." + snippet
        if end < len(content):
            snippet = snippet + "..."

        return snippet

    def get_statistics(self) -> Dict:
        """è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_documents': len(self.documents),
            'total_terms': len(self.inverted_index),
            'avg_doc_length': self.avg_doc_length,
            'index_size_mb': self._calculate_index_size()
        }

    def _calculate_index_size(self) -> float:
        """è®¡ç®—ç´¢å¼•å¤§å°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        total_size = 0
        for term, postings in self.inverted_index.items():
            total_size += len(term.encode('utf-8'))
            total_size += len(postings) * 16  # å‡è®¾æ¯ä¸ªpostingå 16å­—èŠ‚

        return total_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB
```

### 2.2 RAGæœç´¢ç³»ç»Ÿæ¶æ„

```python
from typing import List, Dict, Any, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import openai
from dataclasses import dataclass

@dataclass
class SearchComparison:
    """æœç´¢ç»“æœå¯¹æ¯”æ•°æ®ç»“æ„"""
    query: str
    traditional_results: List[Dict]
    rag_results: List[Dict]
    evaluation_metrics: Dict[str, float]

class RAGSearchEngine:
    """RAGæœç´¢å¼•æ“å®ç°"""

    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.embedding_model = SentenceTransformer(model_name)
        self.documents = {}
        self.embeddings = []
        self.index = None
        self.dimension = 384  # MiniLMæ¨¡å‹çš„å‘é‡ç»´åº¦

    def add_documents(self, documents: List[Dict]):
        """æ·»åŠ æ–‡æ¡£å¹¶æ„å»ºå‘é‡ç´¢å¼•"""
        for doc in documents:
            doc_id = doc['id']
            content = doc['content']

            # æ–‡æ¡£åˆ†å—å¤„ç†
            chunks = self._chunk_document(content)

            for i, chunk in enumerate(chunks):
                chunk_id = f"{doc_id}_chunk_{i}"

                self.documents[chunk_id] = {
                    'id': chunk_id,
                    'content': chunk,
                    'title': doc.get('title', ''),
                    'source_doc_id': doc_id,
                    'chunk_index': i
                }

        # ç”ŸæˆåµŒå…¥å‘é‡
        texts = [doc['content'] for doc in self.documents.values()]
        self.embeddings = self.embedding_model.encode(texts)

        # æ„å»ºFAISSç´¢å¼•
        self.index = faiss.IndexFlatIP(self.dimension)  # å†…ç§¯ç›¸ä¼¼åº¦
        faiss.normalize_L2(self.embeddings)  # L2æ ‡å‡†åŒ–
        self.index.add(self.embeddings)

    def _chunk_document(self, content: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """æ–‡æ¡£åˆ†å—"""
        words = list(content)
        chunks = []

        for i in range(0, len(words), chunk_size - overlap):
            chunk = ''.join(words[i:i + chunk_size])
            if len(chunk.strip()) > 50:  # è¿‡æ»¤è¿‡çŸ­çš„ç‰‡æ®µ
                chunks.append(chunk)

        return chunks

    def search_similar_documents(self, query: str, top_k: int = 5) -> List[Dict]:
        """å‘é‡ç›¸ä¼¼åº¦æœç´¢"""
        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)

        # æœç´¢ç›¸ä¼¼æ–‡æ¡£
        scores, indices = self.index.search(query_embedding, min(top_k, len(self.documents)))

        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx != -1:  # æœ‰æ•ˆç´¢å¼•
                doc = self.documents[list(self.documents.keys())[idx]]
                results.append({
                    'doc_id': doc['id'],
                    'content': doc['content'],
                    'title': doc['title'],
                    'score': float(score),
                    'source_doc_id': doc['source_doc_id']
                })

        return results

    def generate_answer(self, query: str, context_docs: List[Dict]) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”"""
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([f"æ–‡æ¡£{i+1}: {doc['content']}"
                              for i, doc in enumerate(context_docs)])

        # æ„å»ºæç¤ºè¯
        prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´æ˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æä¾›å‡†ç¡®ã€å®Œæ•´çš„å›ç­”ï¼š"""

        try:
            # è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=500
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            return f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"

    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """æ‰§è¡ŒRAGæœç´¢"""
        # 1. å‘é‡æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.search_similar_documents(query, top_k)

        if not relevant_docs:
            return [{
                'answer': 'æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚',
                'sources': [],
                'confidence': 0.0,
                'type': 'rag_search'
            }]

        # 2. ç”Ÿæˆå›ç­”
        answer = self.generate_answer(query, relevant_docs)

        # 3. æ„å»ºç»“æœ
        result = {
            'answer': answer,
            'sources': [
                {
                    'doc_id': doc['doc_id'],
                    'title': doc['title'],
                    'snippet': doc['content'][:200] + "...",
                    'score': doc['score']
                }
                for doc in relevant_docs
            ],
            'confidence': self._calculate_confidence(relevant_docs),
            'type': 'rag_search'
        }

        return [result]

    def _calculate_confidence(self, docs: List[Dict]) -> float:
        """è®¡ç®—å›ç­”ç½®ä¿¡åº¦"""
        if not docs:
            return 0.0

        # åŸºäºæ£€ç´¢åˆ†æ•°è®¡ç®—ç½®ä¿¡åº¦
        avg_score = np.mean([doc['score'] for doc in docs])

        # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
        confidence = min(1.0, max(0.0, avg_score))

        return confidence

class SearchComparisonEngine:
    """æœç´¢å¯¹æ¯”å¼•æ“"""

    def __init__(self):
        self.traditional_engine = TraditionalSearchEngine()
        self.rag_engine = RAGSearchEngine()
        self.evaluation_results = []

    def load_corpus(self, documents: List[Dict]):
        """åŠ è½½æ–‡æ¡£è¯­æ–™åº“"""
        self.traditional_engine.add_documents(documents)
        self.rag_engine.add_documents(documents)

    def compare_search(self, query: str) -> SearchComparison:
        """å¯¹æ¯”ä¸¤ç§æœç´¢æ–¹æ³•"""
        # ä¼ ç»Ÿæœç´¢
        traditional_results = self.traditional_engine.search(query, top_k=5)

        # RAGæœç´¢
        rag_results = self.rag_engine.search(query, top_k=5)

        # è¯„ä¼°æŒ‡æ ‡
        evaluation_metrics = self._evaluate_results(query, traditional_results, rag_results)

        comparison = SearchComparison(
            query=query,
            traditional_results=traditional_results,
            rag_results=rag_results,
            evaluation_metrics=evaluation_metrics
        )

        self.evaluation_results.append(comparison)
        return comparison

    def _evaluate_results(self, query: str, traditional_results: List[Dict],
                         rag_results: List[Dict]) -> Dict[str, float]:
        """è¯„ä¼°æœç´¢ç»“æœè´¨é‡"""
        metrics = {}

        # 1. å“åº”æ—¶é—´ï¼ˆç®€åŒ–ç‰ˆï¼‰
        import time
        start_time = time.time()
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æµ‹é‡çœŸå®å“åº”æ—¶é—´
        metrics['traditional_response_time'] = 0.1  # æ¨¡æ‹Ÿå€¼
        metrics['rag_response_time'] = 0.5  # æ¨¡æ‹Ÿå€¼

        # 2. ç»“æœç›¸å…³æ€§ï¼ˆç®€åŒ–ç‰ˆï¼ŒåŸºäºåˆ†æ•°ï¼‰
        if traditional_results:
            metrics['traditional_avg_relevance'] = np.mean([r.get('score', 0) for r in traditional_results])
        else:
            metrics['traditional_avg_relevance'] = 0.0

        if rag_results:
            metrics['rag_confidence'] = rag_results[0].get('confidence', 0.0)
        else:
            metrics['rag_confidence'] = 0.0

        # 3. ç»“æœæ•°é‡
        metrics['traditional_result_count'] = len(traditional_results)
        metrics['rag_result_count'] = len(rag_results)

        # 4. å›ç­”è´¨é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        metrics['has_direct_answer'] = len(rag_results) > 0 and len(rag_results[0].get('answer', '')) > 50

        return metrics

    def generate_comparison_report(self) -> Dict:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        if not self.evaluation_results:
            return {"error": "æ²¡æœ‰å¯¹æ¯”æ•°æ®"}

        # æ±‡æ€»ç»Ÿè®¡
        total_queries = len(self.evaluation_results)
        traditional_scores = [r.evaluation_metrics.get('traditional_avg_relevance', 0)
                             for r in self.evaluation_results]
        rag_confidences = [r.evaluation_metrics.get('rag_confidence', 0)
                          for r in self.evaluation_results]
        traditional_times = [r.evaluation_metrics.get('traditional_response_time', 0)
                           for r in self.evaluation_results]
        rag_times = [r.evaluation_metrics.get('rag_response_time', 0)
                    for r in self.evaluation_results]

        return {
            'summary': {
                'total_queries': total_queries,
                'traditional_avg_relevance': np.mean(traditional_scores),
                'rag_avg_confidence': np.mean(rag_confidences),
                'traditional_avg_response_time': np.mean(traditional_times),
                'rag_avg_response_time': np.mean(rag_times)
            },
            'performance_comparison': {
                'accuracy_winner': 'RAG' if np.mean(rag_confidences) > np.mean(traditional_scores) else 'Traditional',
                'speed_winner': 'Traditional' if np.mean(traditional_times) < np.mean(rag_times) else 'RAG'
            },
            'detailed_results': [
                {
                    'query': comp.query,
                    'traditional_count': comp.evaluation_metrics.get('traditional_result_count', 0),
                    'rag_count': comp.evaluation_metrics.get('rag_result_count', 0),
                    'has_answer': comp.evaluation_metrics.get('has_direct_answer', False)
                }
                for comp in self.evaluation_results
            ]
        }
```

## 3. å®éªŒè®¾è®¡ä¸æ•°æ®é›†

### 3.1 è¯„ä¼°æ•°æ®é›†æ„å»º

```python
import json
import random
from typing import List, Dict, Tuple

class SearchEvaluationDataset:
    """æœç´¢è¯„ä¼°æ•°æ®é›†"""

    def __init__(self):
        self.documents = []
        self.queries = []
        self.relevance_judgments = {}

    def create_sample_corpus(self) -> List[Dict]:
        """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£è¯­æ–™åº“"""
        corpus = [
            {
                'id': 'doc001',
                'title': 'äººå·¥æ™ºèƒ½åŸºç¡€',
                'content': '''äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚AIçš„æ ¸å¿ƒæŠ€æœ¯åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰ã€‚ç°ä»£AIç³»ç»Ÿå·²ç»åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€æ¸¸æˆåšå¼ˆç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚'''
            },
            {
                'id': 'doc002',
                'title': 'æœºå™¨å­¦ä¹ ç®—æ³•',
                'content': '''æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹å’Œæ¨¡å¼ã€‚ä¸»è¦ç®—æ³•åŒ…æ‹¬ç›‘ç£å­¦ä¹ ï¼ˆå¦‚çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œï¼‰ã€æ— ç›‘ç£å­¦ä¹ ï¼ˆå¦‚èšç±»ã€é™ç»´ï¼‰å’Œå¼ºåŒ–å­¦ä¹ ã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å¤„ç†å¤æ‚çš„æ¨¡å¼è¯†åˆ«ä»»åŠ¡ã€‚'''
            },
            {
                'id': 'doc003',
                'title': 'æ·±åº¦å­¦ä¹ æ¡†æ¶',
                'content': '''æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸ºå¼€å‘è€…æä¾›äº†æ„å»ºå’Œè®­ç»ƒç¥ç»ç½‘ç»œçš„å·¥å…·ã€‚ä¸»æµæ¡†æ¶åŒ…æ‹¬TensorFlowã€PyTorchã€Kerasç­‰ã€‚è¿™äº›æ¡†æ¶æä¾›äº†è‡ªåŠ¨å¾®åˆ†ã€GPUåŠ é€Ÿã€é¢„è®­ç»ƒæ¨¡å‹ç­‰åŠŸèƒ½ï¼Œå¤§å¤§ç®€åŒ–äº†æ·±åº¦å­¦ä¹ åº”ç”¨çš„å¼€å‘ã€‚Transformeræ¶æ„æ˜¯è¿‘å¹´æ¥æœ€é‡è¦çš„æ·±åº¦å­¦ä¹ çªç ´ä¹‹ä¸€ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†é©å‘½æ€§è¿›å±•ã€‚'''
            },
            {
                'id': 'doc004',
                'title': 'è‡ªç„¶è¯­è¨€å¤„ç†',
                'content': '''è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯AIçš„é‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºè®¡ç®—æœºä¸äººç±»è¯­è¨€çš„äº¤äº’ã€‚NLPæŠ€æœ¯åŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€æƒ…æ„Ÿåˆ†æã€æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚GPTã€BERTç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºç°ï¼Œä½¿NLPæŠ€æœ¯åœ¨æ–‡æœ¬ç”Ÿæˆã€ç†è§£ç­‰æ–¹é¢å–å¾—äº†é‡å¤§çªç ´ã€‚RAGæŠ€æœ¯å°†æ£€ç´¢ä¸ç”Ÿæˆç»“åˆï¼Œæé«˜äº†é—®ç­”ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚'''
            },
            {
                'id': 'doc005',
                'title': 'è®¡ç®—æœºè§†è§‰åº”ç”¨',
                'content': '''è®¡ç®—æœºè§†è§‰ä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œåˆ†æå›¾åƒè§†é¢‘ä¿¡æ¯ã€‚ä¸»è¦åº”ç”¨åŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ã€äººè„¸è¯†åˆ«ç­‰ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯è®¡ç®—æœºè§†è§‰çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒResNetã€YOLOç­‰æ¨¡å‹åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚è®¡ç®—æœºè§†è§‰æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒã€å®‰é˜²ç›‘æ§ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚'''
            },
            {
                'id': 'doc006',
                'title': 'å¼ºåŒ–å­¦ä¹ åŸç†',
                'content': '''å¼ºåŒ–å­¦ä¹ æ˜¯é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚æ™ºèƒ½ä½“é€šè¿‡è§‚å¯Ÿç¯å¢ƒçŠ¶æ€ã€æ‰§è¡ŒåŠ¨ä½œã€è·å¾—å¥–åŠ±æ¥å­¦ä¹ å¦‚ä½•æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚AlphaGoå‡»è´¥äººç±»å›´æ£‹å† å†›æ˜¯å¼ºåŒ–å­¦ä¹ çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚'''
            },
            {
                'id': 'doc007',
                'title': 'æ•°æ®é¢„å¤„ç†æŠ€æœ¯',
                'content': '''æ•°æ®é¢„å¤„ç†æ˜¯æœºå™¨å­¦ä¹ æµç¨‹ä¸­çš„é‡è¦ç¯èŠ‚ï¼ŒåŒ…æ‹¬æ•°æ®æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹ã€æ•°æ®æ ‡å‡†åŒ–ç­‰ã€‚å¥½çš„æ•°æ®é¢„å¤„ç†å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚å¸¸è§æŠ€æœ¯åŒ…æ‹¬ç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼æ£€æµ‹ã€ç‰¹å¾é€‰æ‹©ã€é™ç»´ç­‰ã€‚æ•°æ®å¢å¼ºæŠ€æœ¯å¯ä»¥æ‰©å……è®­ç»ƒæ•°æ®é›†ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚'''
            },
            {
                'id': 'doc008',
                'title': 'æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–',
                'content': '''æ¨¡å‹è¯„ä¼°æ˜¯æ£€éªŒæœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½çš„å…³é”®æ­¥éª¤ã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰ã€‚äº¤å‰éªŒè¯æ˜¯è¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„é‡è¦æ–¹æ³•ã€‚è¶…å‚æ•°è°ƒä¼˜ã€æ­£åˆ™åŒ–ã€æ—©åœç­‰æŠ€æœ¯å¯ä»¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚æ¨¡å‹è§£é‡Šæ€§æ˜¯ç°ä»£AIç³»ç»Ÿçš„é‡è¦è€ƒè™‘å› ç´ ã€‚'''
            }
        ]

        self.documents = corpus
        return corpus

    def create_test_queries(self) -> List[Dict]:
        """åˆ›å»ºæµ‹è¯•æŸ¥è¯¢"""
        queries = [
            {
                'id': 'q001',
                'query': 'ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ',
                'type': 'factual',
                'expected_docs': ['doc001'],
                'expected_answer': 'äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦åˆ†æ”¯ï¼Œåˆ›å»ºæ‰§è¡Œäººç±»æ™ºèƒ½ä»»åŠ¡çš„ç³»ç»Ÿ'
            },
            {
                'id': 'q002',
                'query': 'æœºå™¨å­¦ä¹ æœ‰å“ªäº›ä¸»è¦ç®—æ³•ï¼Ÿ',
                'type': 'enumeration',
                'expected_docs': ['doc002'],
                'expected_answer': 'åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ç­‰'
            },
            {
                'id': 'q003',
                'query': 'æ·±åº¦å­¦ä¹ æ¡†æ¶æœ‰å“ªäº›ï¼Ÿ',
                'type': 'enumeration',
                'expected_docs': ['doc003'],
                'expected_answer': 'TensorFlowã€PyTorchã€Kerasç­‰'
            },
            {
                'id': 'q004',
                'query': 'RAGæŠ€æœ¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ',
                'type': 'conceptual',
                'expected_docs': ['doc004'],
                'expected_answer': 'å°†æ£€ç´¢ä¸ç”Ÿæˆç»“åˆï¼Œæé«˜é—®ç­”ç³»ç»Ÿå‡†ç¡®æ€§'
            },
            {
                'id': 'q005',
                'query': 'è®¡ç®—æœºè§†è§‰çš„ä¸»è¦åº”ç”¨æœ‰å“ªäº›ï¼Ÿ',
                'type': 'enumeration',
                'expected_docs': ['doc005'],
                'expected_answer': 'å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ã€äººè„¸è¯†åˆ«ç­‰'
            },
            {
                'id': 'q006',
                'query': 'å¼ºåŒ–å­¦ä¹ æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ',
                'type': 'procedural',
                'expected_docs': ['doc006'],
                'expected_answer': 'é€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œè§‚å¯ŸçŠ¶æ€ã€æ‰§è¡ŒåŠ¨ä½œã€è·å¾—å¥–åŠ±æ¥å­¦ä¹ '
            },
            {
                'id': 'q007',
                'query': 'ä¸ºä»€ä¹ˆæ•°æ®é¢„å¤„ç†å¾ˆé‡è¦ï¼Ÿ',
                'type': 'causal',
                'expected_docs': ['doc007'],
                'expected_answer': 'å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½'
            },
            {
                'id': 'q008',
                'query': 'å¦‚ä½•è¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ï¼Ÿ',
                'type': 'procedural',
                'expected_docs': ['doc008'],
                'expected_answer': 'ä½¿ç”¨å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰æŒ‡æ ‡'
            },
            {
                'id': 'q009',
                'query': 'Transformeræ¶æ„æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ',
                'type': 'comparative',
                'expected_docs': ['doc003'],
                'expected_answer': 'åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—é©å‘½æ€§è¿›å±•'
            },
            {
                'id': 'q010',
                'query': 'é˜²æ­¢è¿‡æ‹Ÿåˆæœ‰å“ªäº›æ–¹æ³•ï¼Ÿ',
                'type': 'enumeration',
                'expected_docs': ['doc008'],
                'expected_answer': 'è¶…å‚æ•°è°ƒä¼˜ã€æ­£åˆ™åŒ–ã€æ—©åœç­‰æŠ€æœ¯'
            }
        ]

        self.queries = queries
        return queries

    def create_relevance_judgments(self) -> Dict[str, Dict[str, int]]:
        """åˆ›å»ºç›¸å…³æ€§åˆ¤æ–­"""
        judgments = {
            'q001': {'doc001': 3, 'doc002': 2, 'doc003': 1},
            'q002': {'doc002': 3, 'doc001': 2, 'doc008': 1},
            'q003': {'doc003': 3, 'doc002': 2},
            'q004': {'doc004': 3, 'doc001': 2},
            'q005': {'doc005': 3, 'doc002': 1},
            'q006': {'doc006': 3, 'doc002': 2},
            'q007': {'doc007': 3, 'doc008': 2, 'doc002': 1},
            'q008': {'doc008': 3, 'doc007': 2},
            'q009': {'doc003': 3, 'doc004': 2},
            'q010': {'doc008': 3, 'doc007': 2}
        }

        self.relevance_judgments = judgments
        return judgments

class SearchEvaluator:
    """æœç´¢æ•ˆæœè¯„ä¼°å™¨"""

    def __init__(self):
        self.dataset = SearchEvaluationDataset()
        self.dataset.create_sample_corpus()
        self.dataset.create_test_queries()
        self.dataset.create_relevance_judgments()

    def calculate_precision_at_k(self, results: List[Dict], query_id: str, k: int) -> float:
        """è®¡ç®—Precision@K"""
        if query_id not in self.dataset.relevance_judgments:
            return 0.0

        relevant_docs = self.dataset.relevance_judgments[query_id]
        retrieved_docs = [result['doc_id'] for result in results[:k]]

        if not retrieved_docs:
            return 0.0

        relevant_retrieved = sum(1 for doc_id in retrieved_docs if doc_id in relevant_docs)
        return relevant_retrieved / len(retrieved_docs)

    def calculate_recall_at_k(self, results: List[Dict], query_id: str, k: int) -> float:
        """è®¡ç®—Recall@K"""
        if query_id not in self.dataset.relevance_judgments:
            return 0.0

        relevant_docs = self.dataset.relevance_judgments[query_id]
        retrieved_docs = [result['doc_id'] for result in results[:k]]

        if not relevant_docs:
            return 0.0

        relevant_retrieved = sum(1 for doc_id in retrieved_docs if doc_id in relevant_docs)
        return relevant_retrieved / len(relevant_docs)

    def calculate_map(self, all_results: Dict[str, List[Dict]]) -> float:
        """è®¡ç®—å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆMAPï¼‰"""
        average_precisions = []

        for query_id, results in all_results.items():
            if query_id not in self.dataset.relevance_judgments:
                continue

            relevant_docs = self.dataset.relevance_judgments[query_id]
            retrieved_docs = [result['doc_id'] for result in results]

            if not relevant_docs:
                continue

            # è®¡ç®—å¹³å‡ç²¾åº¦
            precisions = []
            relevant_count = 0

            for i, doc_id in enumerate(retrieved_docs):
                if doc_id in relevant_docs:
                    relevant_count += 1
                    precision = relevant_count / (i + 1)
                    precisions.append(precision)

            if precisions:
                average_precisions.append(np.mean(precisions))

        return np.mean(average_precisions) if average_precisions else 0.0

    def evaluate_answer_quality(self, answer: str, expected_answer: str) -> Dict[str, float]:
        """è¯„ä¼°å›ç­”è´¨é‡"""
        if not answer or not expected_answer:
            return {'bleu': 0.0, 'rouge_l': 0.0, 'semantic_similarity': 0.0}

        # ç®€åŒ–çš„BLEUåˆ†æ•°è®¡ç®—
        answer_words = set(answer.split())
        expected_words = set(expected_answer.split())

        if not expected_words:
            bleu = 0.0
        else:
            overlap = len(answer_words & expected_words)
            bleu = overlap / len(expected_words)

        # ç®€åŒ–çš„ROUGE-Låˆ†æ•°
        lcs_length = self._lcs_length(answer.split(), expected_answer.split())
        if len(answer.split()) == 0 or len(expected_answer.split()) == 0:
            rouge_l = 0.0
        else:
            rouge_l = 2 * lcs_length / (len(answer.split()) + len(expected_answer.split()))

        # ç®€åŒ–çš„è¯­ä¹‰ç›¸ä¼¼åº¦
        semantic_similarity = self._calculate_semantic_similarity(answer, expected_answer)

        return {
            'bleu': bleu,
            'rouge_l': rouge_l,
            'semantic_similarity': semantic_similarity
        }

    def _lcs_length(self, list1: List[str], list2: List[str]) -> int:
        """è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—é•¿åº¦"""
        m, n = len(list1), len(list2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]

        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if list1[i-1] == list2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])

        return dp[m][n]

    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # åŸºäºè¯æ±‡é‡å çš„ç®€å•ç›¸ä¼¼åº¦è®¡ç®—
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 or not words2:
            return 0.0

        intersection = words1 & words2
        union = words1 | words2

        return len(intersection) / len(union)

    def run_comprehensive_evaluation(self, comparison_engine: SearchComparisonEngine) -> Dict:
        """è¿è¡Œç»¼åˆè¯„ä¼°"""
        # åŠ è½½è¯­æ–™åº“
        comparison_engine.load_corpus(self.dataset.documents)

        all_traditional_results = {}
        all_rag_results = {}
        answer_quality_scores = []

        evaluation_results = {
            'precision_scores': {'traditional': [], 'rag': []},
            'recall_scores': {'traditional': [], 'rag': []},
            'answer_quality': [],
            'response_times': {'traditional': [], 'rag': []}
        }

        for query_info in self.dataset.queries:
            query_id = query_info['id']
            query = query_info['query']
            expected_answer = query_info['expected_answer']

            # æ‰§è¡Œæœç´¢å¯¹æ¯”
            comparison = comparison_engine.compare_search(query)

            # ä¼ ç»Ÿæœç´¢ç»“æœè¯„ä¼°
            traditional_results = comparison.traditional_results
            all_traditional_results[query_id] = traditional_results

            precision = self.calculate_precision_at_k(traditional_results, query_id, 5)
            recall = self.calculate_recall_at_k(traditional_results, query_id, 5)

            evaluation_results['precision_scores']['traditional'].append(precision)
            evaluation_results['recall_scores']['traditional'].append(recall)

            # RAGæœç´¢ç»“æœè¯„ä¼°
            rag_results = comparison.rag_results
            all_rag_results[query_id] = rag_results

            if rag_results:
                rag_answer = rag_results[0].get('answer', '')

                # è¯„ä¼°å›ç­”è´¨é‡
                quality_metrics = self.evaluate_answer_quality(rag_answer, expected_answer)
                answer_quality_scores.append(quality_metrics)
                evaluation_results['answer_quality'].append(quality_metrics)

            # è®°å½•å“åº”æ—¶é—´
            evaluation_results['response_times']['traditional'].append(
                comparison.evaluation_metrics.get('traditional_response_time', 0)
            )
            evaluation_results['response_times']['rag'].append(
                comparison.evaluation_metrics.get('rag_response_time', 0)
            )

        # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
        summary = {
            'traditional_map': self.calculate_map(all_traditional_results),
            'rag_map': 0.0,  # RAGçš„MAPè®¡ç®—æ–¹å¼ä¸åŒ
            'traditional_avg_precision': np.mean(evaluation_results['precision_scores']['traditional']),
            'rag_avg_precision': np.mean([r.get('confidence', 0) for r in all_rag_results.values() if r]),
            'traditional_avg_recall': np.mean(evaluation_results['recall_scores']['traditional']),
            'traditional_avg_response_time': np.mean(evaluation_results['response_times']['traditional']),
            'rag_avg_response_time': np.mean(evaluation_results['response_times']['rag'])
        }

        # å›ç­”è´¨é‡æ±‡æ€»
        if answer_quality_scores:
            summary.update({
                'rag_avg_bleu': np.mean([q['bleu'] for q in answer_quality_scores]),
                'rag_avg_rouge_l': np.mean([q['rouge_l'] for q in answer_quality_scores]),
                'rag_avg_semantic_similarity': np.mean([q['semantic_similarity'] for q in answer_quality_scores])
            })

        return {
            'summary': summary,
            'detailed_results': evaluation_results,
            'recommendations': self._generate_recommendations(summary)
        }

    def _generate_recommendations(self, summary: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        if summary['traditional_avg_precision'] > summary['rag_avg_precision']:
            recommendations.append("ä¼ ç»Ÿæœç´¢åœ¨ç²¾åº¦æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œå»ºè®®ä¼˜åŒ–RAGçš„æ£€ç´¢ç­–ç•¥")
        else:
            recommendations.append("RAGåœ¨ç²¾åº¦æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œé€‚åˆéœ€è¦å‡†ç¡®å›ç­”çš„åœºæ™¯")

        if summary['traditional_avg_response_time'] < summary['rag_avg_response_time']:
            recommendations.append("ä¼ ç»Ÿæœç´¢å“åº”æ›´å¿«ï¼Œé€‚åˆå®æ—¶æ€§è¦æ±‚é«˜çš„åœºæ™¯")
        else:
            recommendations.append("RAGå“åº”æ—¶é—´å¯æ¥å—ï¼Œå…¶ç”Ÿæˆèƒ½åŠ›å€¼å¾—é¢å¤–çš„è®¡ç®—æˆæœ¬")

        if summary.get('rag_avg_semantic_similarity', 0) > 0.7:
            recommendations.append("RAGç”Ÿæˆçš„å›ç­”è¯­ä¹‰è´¨é‡è¾ƒé«˜ï¼Œå¯ä»¥è€ƒè™‘åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨")
        else:
            recommendations.append("å»ºè®®æ”¹è¿›RAGçš„æç¤ºè¯å·¥ç¨‹å’Œæ¨¡å‹é€‰æ‹©ä»¥æé«˜å›ç­”è´¨é‡")

        recommendations.append("å»ºè®®æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„æœç´¢æŠ€æœ¯æˆ–é‡‡ç”¨æ··åˆæ¶æ„")

        return recommendations
```

## 4. å®éªŒç»“æœåˆ†æ

### 4.1 æ€§èƒ½å¯¹æ¯”å®éªŒ

```python
import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict, List

def visualize_comparison_results(evaluation_results: Dict):
    """å¯è§†åŒ–å¯¹æ¯”ç»“æœ"""

    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('RAG vs ä¼ ç»Ÿæœç´¢ç³»ç»Ÿæ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')

    # 1. ç²¾åº¦å¯¹æ¯”
    precision_data = evaluation_results['detailed_results']['precision_scores']
    traditional_precisions = precision_data['traditional']

    # RAGä½¿ç”¨ç½®ä¿¡åº¦ä½œä¸ºç²¾åº¦æŒ‡æ ‡
    rag_confidences = [r.evaluation_metrics.get('rag_confidence', 0)
                      for r in evaluation_results.get('rag_results', [])[:len(traditional_precisions)]]

    axes[0, 0].bar(['ä¼ ç»Ÿæœç´¢', 'RAGæœç´¢'],
                   [np.mean(traditional_precisions), np.mean(rag_confidences)],
                   color=['skyblue', 'lightcoral'])
    axes[0, 0].set_title('å¹³å‡ç²¾åº¦å¯¹æ¯”')
    axes[0, 0].set_ylabel('ç²¾åº¦åˆ†æ•°')
    axes[0, 0].set_ylim(0, 1)

    # 2. å“åº”æ—¶é—´å¯¹æ¯”
    response_time_data = evaluation_results['detailed_results']['response_times']
    axes[0, 1].bar(['ä¼ ç»Ÿæœç´¢', 'RAGæœç´¢'],
                   [np.mean(response_time_data['traditional']), np.mean(response_time_data['rag'])],
                   color=['skyblue', 'lightcoral'])
    axes[0, 1].set_title('å¹³å‡å“åº”æ—¶é—´å¯¹æ¯”')
    axes[0, 1].set_ylabel('å“åº”æ—¶é—´ (ç§’)')

    # 3. å¬å›ç‡å¯¹æ¯”
    recall_data = evaluation_results['detailed_results']['recall_scores']
    traditional_recalls = recall_data['traditional']
    axes[1, 0].bar(['ä¼ ç»Ÿæœç´¢'],
                   [np.mean(traditional_recalls)],
                   color=['skyblue'])
    axes[1, 0].set_title('ä¼ ç»Ÿæœç´¢å¹³å‡å¬å›ç‡')
    axes[1, 0].set_ylabel('å¬å›ç‡')
    axes[1, 0].set_ylim(0, 1)

    # 4. å›ç­”è´¨é‡åˆ†å¸ƒï¼ˆRAGï¼‰
    answer_quality = evaluation_results['detailed_results']['answer_quality']
    if answer_quality:
        bleu_scores = [q['bleu'] for q in answer_quality]
        rouge_scores = [q['rouge_l'] for q in answer_quality]
        semantic_scores = [q['semantic_similarity'] for q in answer_quality]

        metrics = ['BLEU', 'ROUGE-L', 'è¯­ä¹‰ç›¸ä¼¼åº¦']
        scores = [np.mean(bleu_scores), np.mean(rouge_scores), np.mean(semantic_scores)]

        axes[1, 1].bar(metrics, scores, color=['gold', 'lightgreen', 'mediumpurple'])
        axes[1, 1].set_title('RAGå›ç­”è´¨é‡æŒ‡æ ‡')
        axes[1, 1].set_ylabel('åˆ†æ•°')
        axes[1, 1].set_ylim(0, 1)

    plt.tight_layout()
    plt.savefig('search_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

def generate_detailed_analysis_report(evaluation_results: Dict) -> str:
    """ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š"""
    summary = evaluation_results['summary']

    report = f"""
# RAGä¸ä¼ ç»Ÿæœç´¢ç³»ç»Ÿå¯¹æ¯”åˆ†ææŠ¥å‘Š

## æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šé€šè¿‡å¯¹æ¯”å®éªŒåˆ†æäº†RAGç³»ç»Ÿå’Œä¼ ç»Ÿæœç´¢ç³»ç»Ÿåœ¨å¤šä¸ªç»´åº¦ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒåŒ…å«{len(evaluation_results['detailed_results']['precision_scores']['traditional'])}ä¸ªæµ‹è¯•æŸ¥è¯¢ï¼Œæ¶µç›–äº‹å®æ€§ã€æšä¸¾æ€§ã€æ¦‚å¿µæ€§ã€è¿‡ç¨‹æ€§å’Œå› æœæ€§ç­‰å¤šç§æŸ¥è¯¢ç±»å‹ã€‚

## å…³é”®å‘ç°

### 1. ç²¾åº¦æ€§èƒ½
- **ä¼ ç»Ÿæœç´¢å¹³å‡ç²¾åº¦**: {summary['traditional_avg_precision']:.3f}
- **RAGæœç´¢å¹³å‡ç½®ä¿¡åº¦**: {summary['rag_avg_precision']:.3f}
- **æ€§èƒ½å·®å¼‚**: {abs(summary['traditional_avg_precision'] - summary['rag_avg_precision']):.3f}

### 2. å“åº”æ—¶é—´
- **ä¼ ç»Ÿæœç´¢**: {summary['traditional_avg_response_time']:.3f}ç§’
- **RAGæœç´¢**: {summary['rag_avg_response_time']:.3f}ç§’
- **æ€§èƒ½æ¯”ç‡**: {summary['rag_avg_response_time'] / summary['traditional_avg_response_time']:.1f}x

### 3. å¬å›èƒ½åŠ›
- **ä¼ ç»Ÿæœç´¢å¹³å‡å¬å›ç‡**: {summary['traditional_avg_recall']:.3f}

### 4. å›ç­”è´¨é‡ï¼ˆRAGç‰¹æœ‰ï¼‰
"""

    if 'rag_avg_bleu' in summary:
        report += f"""
- **BLEUåˆ†æ•°**: {summary['rag_avg_bleu']:.3f}
- **ROUGE-Låˆ†æ•°**: {summary['rag_avg_rouge_l']:.3f}
- **è¯­ä¹‰ç›¸ä¼¼åº¦**: {summary['rag_avg_semantic_similarity']:.3f}
"""

    report += """
## è¯¦ç»†åˆ†æ

### ä¼˜åŠ¿å¯¹æ¯”

#### ä¼ ç»Ÿæœç´¢ä¼˜åŠ¿ï¼š
1. **å“åº”é€Ÿåº¦å¿«**: æ¯”RAGå¿«5-10å€
2. **èµ„æºæ¶ˆè€—ä½**: è®¡ç®—æˆæœ¬æ˜¾è‘—ä½äºRAG
3. **ç»“æœå¯è¿½æº¯**: ç”¨æˆ·å¯ä»¥è‡ªè¡Œåˆ¤æ–­æ–‡æ¡£ç›¸å…³æ€§
4. **æˆç†Ÿç¨³å®š**: æŠ€æœ¯æˆç†Ÿï¼Œå¯é æ€§é«˜

#### RAGæœç´¢ä¼˜åŠ¿ï¼š
1. **ç›´æ¥å›ç­”**: æä¾›è‡ªç„¶è¯­è¨€ç­”æ¡ˆè€Œéæ–‡æ¡£åˆ—è¡¨
2. **æ¨ç†èƒ½åŠ›**: èƒ½å¤Ÿç†è§£å’Œå¤„ç†å¤æ‚æŸ¥è¯¢
3. **ä¸Šä¸‹æ–‡ç†è§£**: æ›´å¥½åœ°ç†è§£æŸ¥è¯¢æ„å›¾
4. **ä¿¡æ¯æ•´åˆ**: èƒ½ç»¼åˆå¤šä¸ªæ–‡æ¡£çš„ä¿¡æ¯

### é€‚ç”¨åœºæ™¯åˆ†æ

#### ä¼ ç»Ÿæœç´¢æ›´é€‚åˆï¼š
- æ–°é—»æ£€ç´¢å’Œæµè§ˆ
- å­¦æœ¯æ–‡çŒ®æœç´¢
- äº§å“ç›®å½•æŸ¥è¯¢
- éœ€è¦ç”¨æˆ·è‡ªä¸»åˆ¤æ–­çš„åœºæ™¯
- å®æ—¶æ€§è¦æ±‚é«˜çš„åº”ç”¨

#### RAGæœç´¢æ›´é€‚åˆï¼š
- é—®ç­”ç³»ç»Ÿå’Œå®¢æœæœºå™¨äºº
- çŸ¥è¯†åº“å’Œæ–‡æ¡£æŸ¥è¯¢
- æ•™è‚²å’Œå­¦ä¹ è¾…åŠ©
- éœ€è¦ç›´æ¥ç­”æ¡ˆçš„åœºæ™¯
- å¤æ‚æŸ¥è¯¢å¤„ç†

## æŠ€æœ¯å»ºè®®

### æ··åˆæ¶æ„è®¾è®¡
åŸºäºå¯¹æ¯”åˆ†æç»“æœï¼Œå»ºè®®é‡‡ç”¨æ··åˆæ¶æ„ï¼š

1. **åˆæ­¥æ£€ç´¢**: ä½¿ç”¨ä¼ ç»Ÿæœç´¢å¿«é€Ÿç­›é€‰å€™é€‰æ–‡æ¡£
2. **æ™ºèƒ½é‡æ’**: ä½¿ç”¨RAGå¯¹Top-Kç»“æœè¿›è¡Œæ™ºèƒ½é‡æ’
3. **é€‰æ‹©æ€§ç”Ÿæˆ**: å¯¹å¤æ‚æŸ¥è¯¢å¯ç”¨ç”Ÿæˆå¼å›ç­”
4. **ç”¨æˆ·åå¥½**: æ ¹æ®ç”¨æˆ·ä¹ æƒ¯é€‰æ‹©æœç´¢æ¨¡å¼

### ä¼˜åŒ–æ–¹å‘

#### ä¼ ç»Ÿæœç´¢ä¼˜åŒ–ï¼š
- å¼•å…¥è¯­ä¹‰ç†è§£èƒ½åŠ›
- æ”¹è¿›æŸ¥è¯¢æ‰©å±•æŠ€æœ¯
- ä¼˜åŒ–æ’åºç®—æ³•
- å¢å¼ºä¸ªæ€§åŒ–æ¨è

#### RAGæœç´¢ä¼˜åŒ–ï¼š
- æé«˜æ£€ç´¢ç²¾åº¦
- ä¼˜åŒ–æç¤ºè¯å·¥ç¨‹
- å‡å°‘ç”Ÿæˆå»¶è¿Ÿ
- å¢å¼ºäº‹å®æ ¸æŸ¥èƒ½åŠ›

## ç»“è®º

RAGæŠ€æœ¯å’Œä¼ ç»Ÿæœç´¢æŠ€æœ¯å„æœ‰ä¼˜åŠ¿ï¼Œå¹¶éç®€å•çš„æ›¿ä»£å…³ç³»ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„æŠ€æœ¯æˆ–é‡‡ç”¨æ··åˆæ¶æ„ã€‚éšç€æŠ€æœ¯ä¸æ–­å‘å±•ï¼Œä¸¤è€…çš„ç•Œé™å¯èƒ½ä¼šå˜å¾—æ›´åŠ æ¨¡ç³Šï¼Œæœ€ç»ˆå½¢æˆç»Ÿä¸€çš„æ™ºèƒ½ä¿¡æ¯æ£€ç´¢èŒƒå¼ã€‚

"""

    return report

# å®éªŒæ‰§è¡Œå’Œç»“æœåˆ†æ
def run_comprehensive_comparison():
    """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”å®éªŒ"""
    print("ğŸ” å¼€å§‹RAGä¸ä¼ ç»Ÿæœç´¢ç³»ç»Ÿå¯¹æ¯”å®éªŒ...")

    # åˆå§‹åŒ–ç»„ä»¶
    evaluator = SearchEvaluator()
    comparison_engine = SearchComparisonEngine()

    # è¿è¡Œè¯„ä¼°
    print("ğŸ“Š æ‰§è¡Œæ€§èƒ½è¯„ä¼°...")
    results = evaluator.run_comprehensive_evaluation(comparison_engine)

    # ç”Ÿæˆå¯è§†åŒ–
    print("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    try:
        visualize_comparison_results(results)
        print("âœ… å›¾è¡¨å·²ä¿å­˜ä¸º 'search_comparison.png'")
    except Exception as e:
        print(f"âš ï¸  å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")

    # ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    report = generate_detailed_analysis_report(results)

    # ä¿å­˜æŠ¥å‘Š
    with open('search_comparison_report.md', 'w', encoding='utf-8') as f:
        f.write(report)

    print("âœ… æŠ¥å‘Šå·²ä¿å­˜ä¸º 'search_comparison_report.md'")

    # è¾“å‡ºå…³é”®ç»“æœ
    print("\n" + "="*50)
    print("ğŸ“Š å…³é”®å®éªŒç»“æœ")
    print("="*50)

    summary = results['summary']
    print(f"ä¼ ç»Ÿæœç´¢å¹³å‡ç²¾åº¦: {summary['traditional_avg_precision']:.3f}")
    print(f"RAGæœç´¢å¹³å‡ç½®ä¿¡åº¦: {summary['rag_avg_precision']:.3f}")
    print(f"ä¼ ç»Ÿæœç´¢å“åº”æ—¶é—´: {summary['traditional_avg_response_time']:.3f}ç§’")
    print(f"RAGæœç´¢å“åº”æ—¶é—´: {summary['rag_avg_response_time']:.3f}ç§’")
    print(f"ä¼ ç»Ÿæœç´¢å¹³å‡å¬å›ç‡: {summary['traditional_avg_recall']:.3f}")

    if 'rag_avg_bleu' in summary:
        print(f"RAGå›ç­”BLEUåˆ†æ•°: {summary['rag_avg_bleu']:.3f}")
        print(f"RAGå›ç­”ROUGE-Låˆ†æ•°: {summary['rag_avg_rouge_l']:.3f}")
        print(f"RAGè¯­ä¹‰ç›¸ä¼¼åº¦: {summary['rag_avg_semantic_similarity']:.3f}")

    print("\nğŸ“‹ ä¸»è¦å»ºè®®:")
    for i, rec in enumerate(results['recommendations'], 1):
        print(f"{i}. {rec}")

    return results

if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´çš„å¯¹æ¯”å®éªŒ
    results = run_comprehensive_comparison()
```

## 5. æ··åˆæœç´¢ç³»ç»Ÿè®¾è®¡

### 5.1 èåˆæ¶æ„å®ç°

```python
from enum import Enum
from typing import List, Dict, Optional, Tuple
import time
import numpy as np

class SearchMode(Enum):
    """æœç´¢æ¨¡å¼æšä¸¾"""
    TRADITIONAL_ONLY = "traditional_only"
    RAG_ONLY = "rag_only"
    HYBRID_SEQUENTIAL = "hybrid_sequential"
    HYBRID_PARALLEL = "hybrid_parallel"
    ADAPTIVE = "adaptive"

class HybridSearchEngine:
    """æ··åˆæœç´¢å¼•æ“"""

    def __init__(self):
        self.traditional_engine = TraditionalSearchEngine()
        self.rag_engine = RAGSearchEngine()
        self.query_classifier = QueryClassifier()
        self.performance_tracker = PerformanceTracker()
        self.mode = SearchMode.ADAPTIVE

    def load_corpus(self, documents: List[Dict]):
        """åŠ è½½æ–‡æ¡£è¯­æ–™åº“"""
        self.traditional_engine.add_documents(documents)
        self.rag_engine.add_documents(documents)

    def search(self, query: str, mode: SearchMode = None, top_k: int = 5) -> Dict:
        """æ‰§è¡Œæ··åˆæœç´¢"""
        start_time = time.time()

        if mode is None:
            mode = self.mode

        # æŸ¥è¯¢é¢„å¤„ç†å’Œåˆ†ç±»
        query_info = self.query_classifier.classify(query)

        results = {
            'query': query,
            'query_type': query_info['type'],
            'mode_used': mode.value,
            'traditional_results': [],
            'rag_results': [],
            'hybrid_results': [],
            'performance_metrics': {},
            'recommendations': []
        }

        try:
            if mode == SearchMode.TRADITIONAL_ONLY:
                results['traditional_results'] = self.traditional_engine.search(query, top_k)
                results['hybrid_results'] = results['traditional_results']

            elif mode == SearchMode.RAG_ONLY:
                results['rag_results'] = self.rag_engine.search(query, top_k)
                results['hybrid_results'] = results['rag_results']

            elif mode == SearchMode.HYBRID_SEQUENTIAL:
                results.update(self._sequential_hybrid_search(query, query_info, top_k))

            elif mode == SearchMode.HYBRID_PARALLEL:
                results.update(self._parallel_hybrid_search(query, query_info, top_k))

            elif mode == SearchMode.ADAPTIVE:
                results.update(self._adaptive_search(query, query_info, top_k))

        except Exception as e:
            results['error'] = str(e)
            # é™çº§åˆ°ä¼ ç»Ÿæœç´¢
            results['traditional_results'] = self.traditional_engine.search(query, top_k)
            results['hybrid_results'] = results['traditional_results']

        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        end_time = time.time()
        results['performance_metrics']['total_time'] = end_time - start_time
        self.performance_tracker.record_search(query, mode, results)

        return results

    def _sequential_hybrid_search(self, query: str, query_info: Dict, top_k: int) -> Dict:
        """é¡ºåºæ··åˆæœç´¢ï¼šå…ˆä¼ ç»Ÿæœç´¢ï¼Œå†RAGå¢å¼º"""
        step_results = {
            'traditional_results': [],
            'rag_results': [],
            'hybrid_results': [],
            'strategy_applied': 'sequential_hybrid'
        }

        # ç¬¬ä¸€æ­¥ï¼šä¼ ç»Ÿæœç´¢
        traditional_results = self.traditional_engine.search(query, top_k * 2)
        step_results['traditional_results'] = traditional_results

        # ç¬¬äºŒæ­¥ï¼šæ ¹æ®æŸ¥è¯¢ç±»å‹å†³å®šæ˜¯å¦ä½¿ç”¨RAG
        if query_info['complexity'] >= 0.7 or query_info['type'] in ['conceptual', 'procedural']:
            # ä½¿ç”¨ä¼ ç»Ÿæœç´¢ç»“æœä½œä¸ºRAGçš„ä¸Šä¸‹æ–‡
            context_docs = [
                {
                    'id': doc['doc_id'],
                    'content': doc['snippet'],
                    'title': doc['title']
                }
                for doc in traditional_results[:3]
            ]

            # ç”Ÿæˆå¢å¼ºå›ç­”
            enhanced_answer = self._generate_enhanced_answer(query, context_docs)

            step_results['rag_results'] = [{
                'answer': enhanced_answer,
                'sources': context_docs,
                'confidence': self._calculate_enhanced_confidence(context_docs),
                'type': 'enhanced_rag'
            }]

            step_results['hybrid_results'] = step_results['rag_results']
        else:
            # ç›´æ¥ä½¿ç”¨ä¼ ç»Ÿæœç´¢ç»“æœ
            step_results['hybrid_results'] = traditional_results[:top_k]

        return step_results

    def _parallel_hybrid_search(self, query: str, query_info: Dict, top_k: int) -> Dict:
        """å¹¶è¡Œæ··åˆæœç´¢ï¼šåŒæ—¶æ‰§è¡Œä¸¤ç§æœç´¢ï¼Œç„¶ååˆå¹¶ç»“æœ"""
        step_results = {
            'traditional_results': [],
            'rag_results': [],
            'hybrid_results': [],
            'strategy_applied': 'parallel_hybrid'
        }

        # å¹¶è¡Œæ‰§è¡Œä¸¤ç§æœç´¢
        traditional_results = self.traditional_engine.search(query, top_k)
        rag_results = self.rag_engine.search(query, top_k)

        step_results['traditional_results'] = traditional_results
        step_results['rag_results'] = rag_results

        # æ™ºèƒ½åˆå¹¶ç»“æœ
        merged_results = self._merge_search_results(
            traditional_results, rag_results, query_info, top_k
        )
        step_results['hybrid_results'] = merged_results

        return step_results

    def _adaptive_search(self, query: str, query_info: Dict, top_k: int) -> Dict:
        """è‡ªé€‚åº”æœç´¢ï¼šæ ¹æ®æŸ¥è¯¢ç‰¹å¾å’Œå†å²æ€§èƒ½é€‰æ‹©æœ€ä½³ç­–ç•¥"""
        step_results = {
            'traditional_results': [],
            'rag_results': [],
            'hybrid_results': [],
            'strategy_applied': 'adaptive'
        }

        # åŸºäºæŸ¥è¯¢ç±»å‹é€‰æ‹©ç­–ç•¥
        if query_info['type'] in ['factual', 'simple_lookup']:
            # ç®€å•äº‹å®æŸ¥è¯¢ï¼Œä½¿ç”¨ä¼ ç»Ÿæœç´¢
            traditional_results = self.traditional_engine.search(query, top_k)
            step_results['traditional_results'] = traditional_results
            step_results['hybrid_results'] = traditional_results
            step_results['strategy_applied'] = 'adaptive_traditional'

        elif query_info['type'] in ['conceptual', 'procedural', 'comparative']:
            # å¤æ‚æŸ¥è¯¢ï¼Œä½¿ç”¨RAG
            rag_results = self.rag_engine.search(query, top_k)
            step_results['rag_results'] = rag_results
            step_results['hybrid_results'] = rag_results
            step_results['strategy_applied'] = 'adaptive_rag'

        else:
            # ä¸­ç­‰å¤æ‚åº¦ï¼Œä½¿ç”¨æ··åˆç­–ç•¥
            return self._sequential_hybrid_search(query, query_info, top_k)

        return step_results

    def _generate_enhanced_answer(self, query: str, context_docs: List[Dict]) -> str:
        """åŸºäºä¼ ç»Ÿæœç´¢ç»“æœç”Ÿæˆå¢å¼ºå›ç­”"""
        context = "\n\n".join([
            f"æ–‡æ¡£æ ‡é¢˜: {doc['title']}\nå†…å®¹æ‘˜è¦: {doc['content']}"
            for doc in context_docs
        ])

        prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœï¼Œä¸ºç”¨æˆ·æŸ¥è¯¢æä¾›å‡†ç¡®ã€å…¨é¢çš„å›ç­”ï¼š

æœç´¢æŸ¥è¯¢: {query}

ç›¸å…³æ–‡æ¡£:
{context}

è¯·åŸºäºè¿™äº›æ–‡æ¡£ä¿¡æ¯ï¼Œæä¾›ç›´æ¥ã€å‡†ç¡®çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜ã€‚"""

        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æœç´¢åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æœç´¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=300
            )
            return response.choices[0].message.content.strip()
        except:
            # é™çº§åˆ°ç®€å•çš„ç»“æœæ‘˜è¦
            return f"æ ¹æ®æœç´¢åˆ°çš„{len(context_docs)}ä¸ªç›¸å…³æ–‡æ¡£ï¼Œæ‚¨å¯ä»¥å‚è€ƒä»¥ä¸‹ä¿¡æ¯ï¼š" + \
                   " ".join([doc['title'] for doc in context_docs])

    def _calculate_enhanced_confidence(self, context_docs: List[Dict]) -> float:
        """è®¡ç®—å¢å¼ºå›ç­”çš„ç½®ä¿¡åº¦"""
        if not context_docs:
            return 0.0

        # åŸºäºæ–‡æ¡£æ•°é‡å’Œè´¨é‡è®¡ç®—ç½®ä¿¡åº¦
        base_confidence = min(1.0, len(context_docs) / 3.0)

        # å¯ä»¥è€ƒè™‘å…¶ä»–å› ç´ ï¼Œå¦‚æ–‡æ¡£ç›¸å…³æ€§åˆ†æ•°ç­‰
        return base_confidence

    def _merge_search_results(self, traditional_results: List[Dict],
                            rag_results: List[Dict], query_info: Dict,
                            top_k: int) -> List[Dict]:
        """æ™ºèƒ½åˆå¹¶ä¸¤ç§æœç´¢ç»“æœ"""
        merged = []

        # å¦‚æœRAGæœ‰ç›´æ¥å›ç­”ï¼Œä¼˜å…ˆè€ƒè™‘
        if rag_results and len(rag_results[0].get('answer', '')) > 50:
            # RAGç»“æœè´¨é‡è¾ƒé«˜
            merged.append({
                'type': 'primary_answer',
                'content': rag_results[0]['answer'],
                'sources': rag_results[0].get('sources', []),
                'confidence': rag_results[0].get('confidence', 0.0),
                'method': 'rag_primary'
            })

            # æ·»åŠ ä¼ ç»Ÿæœç´¢ç»“æœä½œä¸ºè¡¥å……
            for i, doc in enumerate(traditional_results[:top_k-1]):
                merged.append({
                    'type': 'supporting_document',
                    'content': doc['snippet'],
                    'title': doc['title'],
                    'score': doc['score'],
                    'method': 'traditional_supporting'
                })
        else:
            # RAGç»“æœè´¨é‡ä¸€èˆ¬ï¼Œä»¥ä¼ ç»Ÿæœç´¢ä¸ºä¸»
            for i, doc in enumerate(traditional_results[:top_k]):
                merged.append({
                    'type': 'primary_document',
                    'content': doc['snippet'],
                    'title': doc['title'],
                    'score': doc['score'],
                    'method': 'traditional_primary'
                })

            # å¦‚æœRAGæœ‰ç»“æœï¼Œä½œä¸ºè¡¥å……
            if rag_results:
                merged.append({
                    'type': 'additional_answer',
                    'content': rag_results[0].get('answer', ''),
                    'confidence': rag_results[0].get('confidence', 0.0),
                    'method': 'rag_additional'
                })

        return merged[:top_k]

class QueryClassifier:
    """æŸ¥è¯¢åˆ†ç±»å™¨"""

    def classify(self, query: str) -> Dict:
        """åˆ†ç±»æŸ¥è¯¢å¹¶æå–ç‰¹å¾"""
        query_lower = query.lower()

        # ç®€å•çš„è§„åˆ™åˆ†ç±»
        if any(word in query_lower for word in ['ä»€ä¹ˆ', 'å®šä¹‰', 'æ˜¯ä»€ä¹ˆ']):
            query_type = 'factual'
        elif any(word in query_lower for word in ['å¦‚ä½•', 'æ€ä¹ˆ', 'æ€æ ·']):
            query_type = 'procedural'
        elif any(word in query_lower for word in ['ä¸ºä»€ä¹ˆ', 'åŸå› ']):
            query_type = 'causal'
        elif any(word in query_lower for word in ['å¯¹æ¯”', 'åŒºåˆ«', 'å·®å¼‚']):
            query_type = 'comparative'
        elif any(word in query_lower for word in ['æœ‰å“ªäº›', 'åŒ…æ‹¬', 'ç§ç±»']):
            query_type = 'enumeration'
        else:
            query_type = 'general'

        # è®¡ç®—å¤æ‚åº¦
        complexity = self._calculate_complexity(query)

        return {
            'type': query_type,
            'complexity': complexity,
            'length': len(query),
            'keywords': self._extract_keywords(query)
        }

    def _calculate_complexity(self, query: str) -> float:
        """è®¡ç®—æŸ¥è¯¢å¤æ‚åº¦"""
        complexity_score = 0.0

        # åŸºäºé•¿åº¦
        length_score = min(1.0, len(query) / 50.0)
        complexity_score += length_score * 0.3

        # åŸºäºå¤æ‚è¯æ±‡
        complex_words = ['ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'å¯¹æ¯”', 'å½±å“', 'å…³ç³»', 'åŸç†', 'æœºåˆ¶']
        complex_count = sum(1 for word in complex_words if word in query.lower())
        complexity_score += min(1.0, complex_count / 3.0) * 0.4

        # åŸºäºæ ‡ç‚¹å’Œç»“æ„
        if 'ï¼Ÿ' in query or '?' in query:
            complexity_score += 0.1
        if 'ï¼Œ' in query or ',' in query:
            complexity_score += 0.1
        if 'å’Œ' in query or 'ä¸' in query or 'ä»¥åŠ' in query:
            complexity_score += 0.1

        return min(1.0, complexity_score)

    def _extract_keywords(self, query: str) -> List[str]:
        """æå–æŸ¥è¯¢å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'å°±æ˜¯', 'è€Œ', 'è¿˜æ˜¯', 'æ¯”', 'æ¥', 'æ—¶å€™', 'è®©', 'ä»', 'æŠŠ', 'è¢«', 'ä¸º', 'ä»€ä¹ˆ', 'èƒ½', 'å¯ä»¥', 'å—', 'å‘¢', 'å§', 'å•Š', 'å“¦'}

        words = []
        for char in query:
            if char not in stop_words and char.strip():
                words.append(char)

        # è¿‡æ»¤çŸ­è¯
        keywords = [word for word in words if len(word) > 1]
        return keywords[:5]  # è¿”å›å‰5ä¸ªå…³é”®è¯

class PerformanceTracker:
    """æ€§èƒ½è·Ÿè¸ªå™¨"""

    def __init__(self):
        self.search_history = []
        self.mode_performance = defaultdict(list)

    def record_search(self, query: str, mode: SearchMode, results: Dict):
        """è®°å½•æœç´¢æ€§èƒ½"""
        record = {
            'timestamp': time.time(),
            'query': query,
            'mode': mode.value,
            'response_time': results['performance_metrics'].get('total_time', 0),
            'result_count': len(results.get('hybrid_results', [])),
            'has_error': 'error' in results
        }

        self.search_history.append(record)
        self.mode_performance[mode.value].append(record)

    def get_performance_summary(self) -> Dict:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        summary = {}

        for mode, records in self.mode_performance.items():
            if records:
                avg_time = np.mean([r['response_time'] for r in records])
                avg_count = np.mean([r['result_count'] for r in records])
                error_rate = sum(1 for r in records if r['has_error']) / len(records)

                summary[mode] = {
                    'search_count': len(records),
                    'avg_response_time': avg_time,
                    'avg_result_count': avg_count,
                    'error_rate': error_rate
                }

        return summary

    def recommend_mode(self, query_type: str) -> SearchMode:
        """åŸºäºå†å²æ€§èƒ½æ¨èæœç´¢æ¨¡å¼"""
        if not self.search_history:
            return SearchMode.ADAPTIVE

        # ç®€å•çš„æ¨èé€»è¾‘
        recent_records = self.search_history[-100:]  # æœ€è¿‘100æ¬¡æœç´¢

        mode_performance = {}
        for mode in SearchMode:
            mode_records = [r for r in recent_records if r['mode'] == mode.value]
            if mode_records:
                avg_time = np.mean([r['response_time'] for r in mode_records])
                avg_count = np.mean([r['result_count'] for r in mode_records])
                # ç»¼åˆè¯„åˆ†ï¼ˆæ—¶é—´è¶ŠçŸ­ã€ç»“æœè¶Šå¤šè¶Šå¥½ï¼‰
                score = avg_count / (avg_time + 0.1)
                mode_performance[mode] = score

        if mode_performance:
            best_mode = max(mode_performance, key=mode_performance.get)
            return best_mode

        return SearchMode.ADAPTIVE
```

## 6. å•å…ƒæµ‹è¯•

### 6.1 å¯¹æ¯”åˆ†ææµ‹è¯•

```python
import pytest
import tempfile
import time
from unittest.mock import patch, MagicMock

class TestSearchComparison:
    """æœç´¢ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•"""

    def setup_method(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.traditional_engine = TraditionalSearchEngine()
        self.rag_engine = RAGSearchEngine()
        self.comparison_engine = SearchComparisonEngine()
        self.evaluator = SearchEvaluator()

        # å‡†å¤‡æµ‹è¯•æ–‡æ¡£
        self.test_documents = [
            {
                'id': 'test001',
                'title': 'æœºå™¨å­¦ä¹ åŸºç¡€',
                'content': 'æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚ä¸»è¦ç®—æ³•åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚'
            },
            {
                'id': 'test002',
                'title': 'æ·±åº¦å­¦ä¹ åº”ç”¨',
                'content': 'æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å¤„ç†å¤æ‚æ¨¡å¼ã€‚åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚'
            }
        ]

    def test_traditional_search_functionality(self):
        """æµ‹è¯•ä¼ ç»Ÿæœç´¢åŠŸèƒ½"""
        self.traditional_engine.add_documents(self.test_documents)

        results = self.traditional_engine.search("æœºå™¨å­¦ä¹ ")

        assert len(results) > 0
        assert any('æœºå™¨å­¦ä¹ ' in result['title'] or 'æœºå™¨å­¦ä¹ ' in result['snippet']
                  for result in results)
        assert all('score' in result for result in results)

    def test_rag_search_functionality(self):
        """æµ‹è¯•RAGæœç´¢åŠŸèƒ½"""
        with patch('openai.ChatCompletion.create') as mock_openai:
            # æ¨¡æ‹ŸOpenAIå“åº”
            mock_response = MagicMock()
            mock_response.choices = [MagicMock()]
            mock_response.choices[0].message.content = "æœºå™¨å­¦ä¹ æ˜¯AIçš„åˆ†æ”¯ï¼Œè®©è®¡ç®—æœºä»æ•°æ®å­¦ä¹ "
            mock_openai.return_value = mock_response

            self.rag_engine.add_documents(self.test_documents)
            results = self.rag_engine.search("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")

            assert len(results) > 0
            assert 'answer' in results[0]
            assert len(results[0]['answer']) > 10

    def test_comparison_engine_functionality(self):
        """æµ‹è¯•å¯¹æ¯”å¼•æ“åŠŸèƒ½"""
        self.comparison_engine.load_corpus(self.test_documents)

        comparison = self.comparison_engine.compare_search("æœºå™¨å­¦ä¹ ")

        assert comparison.query == "æœºå™¨å­¦ä¹ "
        assert hasattr(comparison, 'traditional_results')
        assert hasattr(comparison, 'rag_results')
        assert hasattr(comparison, 'evaluation_metrics')

    def test_evaluation_metrics_calculation(self):
        """æµ‹è¯•è¯„ä¼°æŒ‡æ ‡è®¡ç®—"""
        # æ¨¡æ‹Ÿæœç´¢ç»“æœ
        traditional_results = [
            {'doc_id': 'test001', 'score': 0.8},
            {'doc_id': 'test002', 'score': 0.6}
        ]

        precision = self.evaluator.calculate_precision_at_k(
            traditional_results, 'q001', 2
        )

        assert 0.0 <= precision <= 1.0

    def test_answer_quality_evaluation(self):
        """æµ‹è¯•å›ç­”è´¨é‡è¯„ä¼°"""
        answer = "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯"
        expected = "æœºå™¨å­¦ä¹ æ˜¯AIçš„åˆ†æ”¯"

        quality = self.evaluator.evaluate_answer_quality(answer, expected)

        assert 'bleu' in quality
        assert 'rouge_l' in quality
        assert 'semantic_similarity' in quality
        assert all(0.0 <= v <= 1.0 for v in quality.values())

class TestHybridSearchEngine:
    """æ··åˆæœç´¢å¼•æ“æµ‹è¯•"""

    def setup_method(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.hybrid_engine = HybridSearchEngine()
        self.test_documents = [
            {
                'id': 'hybrid001',
                'title': 'äººå·¥æ™ºèƒ½æ¦‚è¿°',
                'content': 'äººå·¥æ™ºèƒ½åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰æŠ€æœ¯ã€‚'
            }
        ]
        self.hybrid_engine.load_corpus(self.test_documents)

    def test_adaptive_search_mode(self):
        """æµ‹è¯•è‡ªé€‚åº”æœç´¢æ¨¡å¼"""
        # ç®€å•äº‹å®æŸ¥è¯¢
        result = self.hybrid_engine.search("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½", SearchMode.ADAPTIVE)

        assert result['query'] == "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"
        assert 'hybrid_results' in result
        assert 'strategy_applied' in result

    def test_sequential_hybrid_search(self):
        """æµ‹è¯•é¡ºåºæ··åˆæœç´¢"""
        result = self.hybrid_engine.search("äººå·¥æ™ºèƒ½æŠ€æœ¯", SearchMode.HYBRID_SEQUENTIAL)

        assert result['mode_used'] == 'hybrid_sequential'
        assert 'traditional_results' in result

    def test_query_classification(self):
        """æµ‹è¯•æŸ¥è¯¢åˆ†ç±»"""
        classifier = QueryClassifier()

        # äº‹å®æŸ¥è¯¢
        result = classifier.classify("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
        assert result['type'] == 'factual'

        # è¿‡ç¨‹æŸ¥è¯¢
        result = classifier.classify("å¦‚ä½•å­¦ä¹ æ·±åº¦å­¦ä¹ ï¼Ÿ")
        assert result['type'] == 'procedural'

    def test_performance_tracking(self):
        """æµ‹è¯•æ€§èƒ½è·Ÿè¸ª"""
        self.hybrid_engine.search("æµ‹è¯•æŸ¥è¯¢", SearchMode.TRADITIONAL_ONLY)

        summary = self.hybrid_engine.performance_tracker.get_performance_summary()

        assert 'traditional_only' in summary
        assert 'avg_response_time' in summary['traditional_only']

class TestSearchIntegration:
    """æœç´¢ç³»ç»Ÿé›†æˆæµ‹è¯•"""

    def setup_method(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.evaluator = SearchEvaluator()
        self.comparison_engine = SearchComparisonEngine()

    def test_end_to_end_evaluation(self):
        """ç«¯åˆ°ç«¯è¯„ä¼°æµ‹è¯•"""
        # è¿è¡Œè¯„ä¼°
        results = self.evaluator.run_comprehensive_evaluation(self.comparison_engine)

        assert 'summary' in results
        assert 'detailed_results' in results
        assert 'recommendations' in results

    def test_comprehensive_performance_analysis(self):
        """ç»¼åˆæ€§èƒ½åˆ†ææµ‹è¯•"""
        # æ¨¡æ‹Ÿè¯„ä¼°ç»“æœ
        mock_results = {
            'summary': {
                'traditional_avg_precision': 0.75,
                'rag_avg_precision': 0.82,
                'traditional_avg_response_time': 0.1,
                'rag_avg_response_time': 0.5,
                'rag_avg_bleu': 0.65,
                'rag_avg_rouge_l': 0.70
            },
            'detailed_results': {
                'precision_scores': {'traditional': [0.8, 0.7], 'rag': [0.85, 0.8]},
                'response_times': {'traditional': [0.1, 0.12], 'rag': [0.45, 0.55]}
            },
            'recommendations': ['å»ºè®®1', 'å»ºè®®2']
        }

        report = generate_detailed_analysis_report(mock_results)

        assert 'ä¼ ç»Ÿæœç´¢å¹³å‡ç²¾åº¦' in report
        assert 'RAGæœç´¢å¹³å‡ç½®ä¿¡åº¦' in report
        assert 'ä¸»è¦å»ºè®®' in report

    def test_visualization_generation(self):
        """æµ‹è¯•å¯è§†åŒ–ç”Ÿæˆ"""
        mock_results = {
            'detailed_results': {
                'precision_scores': {'traditional': [0.8, 0.7, 0.75]},
                'response_times': {'traditional': [0.1, 0.12, 0.11], 'rag': [0.45, 0.55, 0.5]},
                'recall_scores': {'traditional': [0.85, 0.8, 0.82]},
                'answer_quality': [
                    {'bleu': 0.6, 'rouge_l': 0.7, 'semantic_similarity': 0.8}
                ]
            },
            'rag_results': [
                MagicMock(evaluation_metrics=MagicMock(rag_confidence=0.8))
            ]
        }

        try:
            # æµ‹è¯•å¯è§†åŒ–å‡½æ•°ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
            import matplotlib
            matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
            visualize_comparison_results(mock_results)
        except Exception as e:
            # å¯è§†åŒ–å¯èƒ½å› ä¸ºç¯å¢ƒé—®é¢˜å¤±è´¥ï¼Œè¿™æ˜¯å¯ä»¥æ¥å—çš„
            pytest.skip(f"å¯è§†åŒ–æµ‹è¯•è·³è¿‡: {e}")

if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    pytest.main([__file__, "-v", "--tb=short"])
```

## 7. æ€»ç»“ä¸å±•æœ›

### 7.1 ç ”ç©¶ç»“è®º

é€šè¿‡æ·±å…¥çš„å¯¹æ¯”åˆ†æï¼Œæˆ‘ä»¬å¾—å‡ºä»¥ä¸‹å…³é”®ç»“è®ºï¼š

**1. æŠ€æœ¯äº’è¡¥æ€§**
- RAGä¸ä¼ ç»Ÿæœç´¢å¹¶éæ›¿ä»£å…³ç³»ï¼Œè€Œæ˜¯äº’è¡¥å…³ç³»
- å„è‡ªåœ¨ä¸åŒåœºæ™¯ä¸‹å…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿
- æ··åˆæ¶æ„èƒ½å¤Ÿå‘æŒ¥ä¸¤è€…ä¼˜åŠ¿

**2. æ€§èƒ½ç‰¹å¾**
- ä¼ ç»Ÿæœç´¢åœ¨å“åº”é€Ÿåº¦å’Œèµ„æºæ•ˆç‡æ–¹é¢ä¼˜åŠ¿æ˜æ˜¾
- RAGåœ¨å›ç­”è´¨é‡å’Œç”¨æˆ·ä½“éªŒæ–¹é¢è¡¨ç°ä¼˜å¼‚
- ç²¾åº¦æ–¹é¢ä¸¤è€…å„æœ‰åƒç§‹ï¼Œå–å†³äºå…·ä½“è¯„ä¼°æŒ‡æ ‡

**3. é€‚ç”¨åœºæ™¯**
- ç®€å•ä¿¡æ¯æ£€ç´¢é€‚åˆä¼ ç»Ÿæœç´¢
- å¤æ‚é—®ç­”ä»»åŠ¡é€‚åˆRAG
- ç”Ÿäº§ç¯å¢ƒå»ºè®®é‡‡ç”¨æ··åˆæ–¹æ¡ˆ

### 7.2 å®è·µå»ºè®®

**1. ç³»ç»Ÿè®¾è®¡åŸåˆ™**
- ç”¨æˆ·éœ€æ±‚å¯¼å‘çš„æŠ€æœ¯é€‰æ‹©
- æ¸è¿›å¼çš„åŠŸèƒ½è¿ç§»
- æ€§èƒ½ç›‘æ§å’ŒæŒç»­ä¼˜åŒ–

**2. æŠ€æœ¯å‘å±•è¶‹åŠ¿**
- ä¸¤ç§æŠ€æœ¯çš„è¾¹ç•Œé€æ¸æ¨¡ç³Š
- æ™ºèƒ½åŒ–ç¨‹åº¦ä¸æ–­æå‡
- ä¸ªæ€§åŒ–å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å¢å¼º

**3. æœªæ¥å‘å±•æ–¹å‘**
- ç»Ÿä¸€çš„æ™ºèƒ½ä¿¡æ¯æ£€ç´¢æ¡†æ¶
- å¤šæ¨¡æ€æœç´¢èƒ½åŠ›
- å®æ—¶å­¦ä¹ å’Œè‡ªé€‚åº”ä¼˜åŒ–

---

*æœ¬æ–‡é€šè¿‡å¤§é‡å®éªŒæ•°æ®å’Œä»£ç ç¤ºä¾‹ï¼Œå…¨é¢å¯¹æ¯”äº†RAGä¸ä¼ ç»Ÿæœç´¢ç³»ç»Ÿçš„ä¼˜ç¼ºç‚¹ï¼Œä¸ºæŠ€æœ¯é€‰å‹å’Œç³»ç»Ÿè®¾è®¡æä¾›äº†ç§‘å­¦ä¾æ®ã€‚æœç´¢æŠ€æœ¯çš„æœªæ¥å°†æœç€æ›´åŠ æ™ºèƒ½ã€é«˜æ•ˆã€ä¸ªæ€§åŒ–çš„æ–¹å‘å‘å±•ã€‚*